{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "RJIsW-Wskuzm",
      "metadata": {
        "id": "RJIsW-Wskuzm"
      },
      "source": [
        "# IFT6135-A2023\n",
        "# Assignment 3: Diffusion Practical\n",
        "\n",
        "You must fill in your answers to various questions in this notebook, following which you must export this notebook to a Python file named `gan.py` and submit it on Gradescope.\n",
        "\n",
        "Only edit the functions specified in the PDF (and wherever marked â€“ `# WRITE CODE HERE`). Do not change definitions or edit the rest of the template, else the autograder will not work.\n",
        "\n",
        "**Make sure you request a GPU runtime!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tzOCo7WPkof2",
      "metadata": {
        "id": "tzOCo7WPkof2"
      },
      "source": [
        "# Diffusion Basics\n",
        "\n",
        "Diffusion models are a new and recent class of generative models that rely on a forward diffusion process and a backward denoising process. The forward diffusion process adds a little bit of noise at each step, thus making the input image progressively noisier. On the other hand, the aim of the backward process is to denoise at each step, and is supposed to reverse the effect of the forward process. In this setup, only the backward process is parameterized by a learnable model while the forward process converges to something known, like $\\mathcal{N}(0, I)$. If the learning is done correctly and the backward process works well enough, it would ideally be able to progressively remove noise from complete noise and lead to a sample from the data distribution.\n",
        "\n",
        "Now, lets try to formalize this. Suppose our data samples come from the distribution $q(x_0)$. The forward distribution can be parameterized as\n",
        "\n",
        "\\begin{align*}\n",
        "q(x_t | x_{t-1}) = \\mathcal{N}(\\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)\n",
        "\\end{align*}\n",
        "\n",
        "where if we consider a finite number of timesteps T, then\n",
        "\n",
        "\\begin{align*}\n",
        "q(x_{1:T} | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\n",
        "\\end{align*}\n",
        "\n",
        "where $\\beta_t$ are the hyperparameters that govern how quickly structure is destroyed in the forward process. One can see this as progressively adding more noise to the input data. One benefit of considering a gaussian distribution above is that it leads to a very nice property that the distribution $q(x_t | x_0)$ becomes known and tractable, so one is able to directly generate a sample from any point in the forward trajectory. In particular, with a bit of algebra, one can obtain\n",
        "\n",
        "\\begin{align*}\n",
        "q(x_t | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_t$ and $\\alpha_t = 1 - \\beta_t$. And ideally we want to set $\\beta_t$'s such that $q(x_T | x_0) \\approx \\mathcal{N}(0, I)$.\n",
        "\n",
        "So far, we have obtained the forward structure-destroying diffusion process as well as how to sample directly from any point in the forward process conditioned on the initial conditions. Now, we want to learn a reverse process that takes us back from a noisy sample to something that has less noise. We do this by parameterizing this distribution with a Neural Network like\n",
        "\n",
        "\\begin{align*}\n",
        "  p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\n",
        "\\end{align*}\n",
        "\n",
        "where we consider $p(x_T)$ as just $\\mathcal{N}(0, I)$ since at the end of the forward diffusion we are approximating that. Further, we also assume that each conditional above is parameterized as a gaussian distribution, that is\n",
        "\n",
        "\\begin{align*}\n",
        "  p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(\\mu_\\theta(x_t, t), \\tilde{\\beta}_t)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$ which is because after doing some algebra, we can find that\n",
        "\n",
        "\\begin{align*}\n",
        "  q(x_{t-1} | x_t, x_0) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\tilde{\\mu}_t = \\frac{\\sqrt{\\alpha_t} ( 1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}x_0$.\n",
        "\n",
        "This particular parameterization of $q(x_{t-1} | x_t, x_0)$ follows through by considering the Bayes rule combined with some algebra. Since the backward learned process is aimed to approximate the true backward process $q(x_{t-1} | x_t, x_0)$, it boils down to matching the means $\\tilde{\\mu}_t$ with $\\mu_\\theta(x_t)$ as the variances are kept the same by design. A little more algebraic manipulation and reparameterization tricks lead us to\n",
        "\n",
        "\\begin{align*}\n",
        "  \\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t\\right)\n",
        "\\end{align*}\n",
        "\n",
        "To match this, we need $\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t\\right)\n",
        "$\n",
        "\n",
        "Given this formulation for $\\mu_\\theta$, we can use reparameterization to get $x_{t-1}$ from $x_t$ as\n",
        "\n",
        "\\begin{align*}\n",
        "  x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t\\right) + \\tilde{\\beta}_t \\epsilon\n",
        "\\end{align*}\n",
        "\n",
        "Thus, instead of parameterizing $\\mu_\\theta$ using a Neural Network, one can parameterize $\\epsilon_t$ using the network so that we can do the backward diffusion as\n",
        "\n",
        "\\begin{align*}\n",
        "  x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)\\right) + \\tilde{\\beta}_t \\epsilon\n",
        "\\end{align*}\n",
        "\n",
        "In short, thus, we get a forward diffusion process parameterized as $q(x_t | x_{t-1} = \\mathcal{N}(\\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$ where one can sample $q(x_t | x_0)$ in one-shot in closed form without having to go through $t = 1, ..., t-1$. On the other hand, the backward process parameterizes the noise at time-step $t$, that is $\\epsilon_\\theta(x_t, t)$ which can be used to run the backward process as $p(x_{t-1} | x_t) = \\mathcal{N}(\\frac{1}{\\sqrt{\\alpha}_t} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} }\\epsilon_\\theta(x_t, t)\\right), \\tilde{\\beta}_t I)$.\n",
        "\n",
        "And learning of this model leads to a simple objective function, which can be defined as\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{t\\sim \\mathcal{U}(1,T), x_0, \\epsilon_t} \\left[|| \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t, t) ||^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "For our settings, instead of the $L_2$ loss, we will use the huber loss, which is $L_1$ loss but near the origin, acts as an $L_2$ loss. For details, refer [here](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html). For more details about diffusion models, please refer to the [DDPM paper](https://arxiv.org/abs/2006.11239) and the related [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f2d714",
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "from torchvision import transforms\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def fix_experiment_seed(seed=0):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_experiment_seed()\n",
        "\n",
        "results_folder = Path(\"./results_diffusion\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uu-S0tNpk56T",
      "metadata": {
        "id": "Uu-S0tNpk56T"
      },
      "source": [
        "## Set up the hyperparameters\n",
        "- Batch Size\n",
        "- Latent Dimensionality\n",
        "- Learning Rate\n",
        "- Diffusion timesteps: $T$\n",
        "- Starting variance: $\\beta_1$\n",
        "- Ending variance: $\\beta_T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rcZzhdIHk6et",
      "metadata": {
        "id": "rcZzhdIHk6et"
      },
      "outputs": [],
      "source": [
        "# Training Hyperparameters\n",
        "batch_size = 64   # Batch Size\n",
        "z_dim = 32        # Latent Dimensionality\n",
        "lr = 1e-4         # Learning Rate\n",
        "\n",
        "# Hyperparameters taken from Ho et. al for noise scheduling\n",
        "T = 1000            # Diffusion Timesteps\n",
        "beta_start = 0.0001 # Starting variance\n",
        "beta_end = 0.02     # Ending variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Djm92b3dlAQ4",
      "metadata": {
        "id": "Djm92b3dlAQ4"
      },
      "outputs": [],
      "source": [
        "# Define Dataset Statistics\n",
        "image_size = 32\n",
        "input_channels = 1\n",
        "\n",
        "# Resize and Normalize the Data\n",
        "transform = Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# Helper Functions\n",
        "def show_image(image, nrow=8):\n",
        "  # Input: image\n",
        "  # Displays the image using matplotlib\n",
        "  grid_img = make_grid(image.detach().cpu(), nrow=nrow, padding=0)\n",
        "  plt.imshow(grid_img.permute(1, 2, 0))\n",
        "  plt.axis('off')\n",
        "\n",
        "def transforms_examples(examples):\n",
        "  # Helper function to perform transformations on the input images\n",
        "  if \"image\" in examples:\n",
        "     examples[\"pixel_values\"] = [transform(image) for image in examples[\"image\"]]\n",
        "     del examples[\"image\"]\n",
        "  else:\n",
        "     examples[\"pixel_values\"] = [transform(image) for image in examples[\"img\"]]\n",
        "     del examples[\"img\"]\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fsKYAJTTlBaQ",
      "metadata": {
        "id": "fsKYAJTTlBaQ"
      },
      "outputs": [],
      "source": [
        "# Load dataset from the hub, normalize it and create the dataloader\n",
        "def get_dataloaders():\n",
        "  dataset = load_dataset(\"mnist\", cache_dir='./data')\n",
        "  transformed_dataset = dataset.with_transform(transforms_examples)\n",
        "  train_dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  test_dataloader = DataLoader(transformed_dataset[\"test\"], batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YHQL39dhlDY-",
      "metadata": {
        "id": "YHQL39dhlDY-"
      },
      "source": [
        "## Visualize the Data\n",
        "\n",
        "Let's visualize what our data actually looks like! We are using the [MNIST](https://huggingface.co/datasets/mnist). The MNIST dataset is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. Please note that you don't need to download dataset yourself as the code we provided download the dataset for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "maXVk2OPlEeY",
      "metadata": {
        "id": "maXVk2OPlEeY"
      },
      "outputs": [],
      "source": [
        "# Visualize the Dataset\n",
        "def visualize():\n",
        "  train_dataloader, _ = get_dataloaders()\n",
        "  batch = next(iter(train_dataloader))\n",
        "  print(batch['pixel_values'].shape)\n",
        "\n",
        "  save_image((batch['pixel_values'] + 1.) * 0.5, './results_diffusion/orig.png')\n",
        "  show_image((batch['pixel_values'] + 1.) * 0.5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uAPXFtg5xCrO",
      "metadata": {
        "id": "uAPXFtg5xCrO"
      },
      "source": [
        "## Helper Functions / Building Blocks\n",
        "\n",
        "Here we provide some helper functions and building blocks that will allow us to create the U-Net network that parameterizes the backward noise prediction network in diffusion models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcoqMfVTTniW",
      "metadata": {
        "id": "bcoqMfVTTniW"
      },
      "outputs": [],
      "source": [
        "def exists(x):\n",
        "  return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "  if exists(val):\n",
        "    return val\n",
        "  return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, *args, **kwargs):\n",
        "    return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "  return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "  return nn.Conv2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "\n",
        "  def forward(self, time):\n",
        "    device = time.device\n",
        "    half_dim = self.dim // 2\n",
        "    embeddings = math.log(10000) / (half_dim - 1)\n",
        "    embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "    embeddings = time[:, None] * embeddings[None, :]\n",
        "    embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "    return embeddings\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, dim, dim_out, groups = 8):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "    self.norm = nn.GroupNorm(groups, dim_out)\n",
        "    self.act = nn.SiLU()\n",
        "\n",
        "  def forward(self, x, scale_shift = None):\n",
        "    x = self.proj(x)\n",
        "    x = self.norm(x)\n",
        "\n",
        "    if exists(scale_shift):\n",
        "      scale, shift = scale_shift\n",
        "      x = x * (scale + 1) + shift\n",
        "\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "  \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "\n",
        "  def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "    super().__init__()\n",
        "    self.mlp = (\n",
        "      nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "      if exists(time_emb_dim)\n",
        "      else None\n",
        "    )\n",
        "\n",
        "    self.block1 = Block(dim, dim_out, groups=groups)\n",
        "    self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "    self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "  def forward(self, x, time_emb=None):\n",
        "    h = self.block1(x)\n",
        "\n",
        "    if exists(self.mlp) and exists(time_emb):\n",
        "      time_emb = self.mlp(time_emb)\n",
        "      h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "    h = self.block2(h)\n",
        "    return h + self.res_conv(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, heads=4, dim_head=32):\n",
        "    super().__init__()\n",
        "    self.scale = dim_head**-0.5\n",
        "    self.heads = heads\n",
        "    hidden_dim = dim_head * heads\n",
        "    self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "    self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, c, h, w = x.shape\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "    q, k, v = map(\n",
        "        lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "    )\n",
        "    q = q * self.scale\n",
        "\n",
        "    sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "    sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "    attn = sim.softmax(dim=-1)\n",
        "\n",
        "    out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "    out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "    return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "  def __init__(self, dim, heads=4, dim_head=32):\n",
        "    super().__init__()\n",
        "    self.scale = dim_head**-0.5\n",
        "    self.heads = heads\n",
        "    hidden_dim = dim_head * heads\n",
        "    self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "    self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
        "                                nn.GroupNorm(1, dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, c, h, w = x.shape\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "    q, k, v = map(\n",
        "      lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "    )\n",
        "\n",
        "    q = q.softmax(dim=-2)\n",
        "    k = k.softmax(dim=-1)\n",
        "\n",
        "    q = q * self.scale\n",
        "    context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "    out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "    out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "    return self.to_out(out)\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "  def __init__(self, dim, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "    self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.norm(x)\n",
        "    return self.fn(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nPAGPDEIUKQF",
      "metadata": {
        "id": "nPAGPDEIUKQF"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      dim,\n",
        "      init_dim=None,\n",
        "      out_dim=None,\n",
        "      dim_mults=(1, 2, 4, 8),\n",
        "      channels=3,\n",
        "      with_time_emb=True,\n",
        "      resnet_block_groups=8,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    # determine dimensions\n",
        "    self.channels = channels\n",
        "\n",
        "    init_dim = default(init_dim, dim // 3 * 2)\n",
        "    self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "    dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "    in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "    block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "    # time embeddings\n",
        "    if with_time_emb:\n",
        "      time_dim = dim * 4\n",
        "      self.time_mlp = nn.Sequential(\n",
        "        SinusoidalPositionEmbeddings(dim),\n",
        "        nn.Linear(dim, time_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(time_dim, time_dim),\n",
        "      )\n",
        "    else:\n",
        "      time_dim = None\n",
        "      self.time_mlp = None\n",
        "\n",
        "    # layers\n",
        "    self.downs = nn.ModuleList([])\n",
        "    self.ups = nn.ModuleList([])\n",
        "    num_resolutions = len(in_out)\n",
        "\n",
        "    for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "      is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "      self.downs.append(\n",
        "        nn.ModuleList(\n",
        "          [\n",
        "            block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "            block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "            Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "            Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "          ]\n",
        "        )\n",
        "      )\n",
        "\n",
        "    mid_dim = dims[-1]\n",
        "    self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "    self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "    self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "    for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "      is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "      self.ups.append(\n",
        "        nn.ModuleList(\n",
        "          [\n",
        "            block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "            block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "            Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "            Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "          ]\n",
        "        )\n",
        "      )\n",
        "\n",
        "    out_dim = default(out_dim, channels)\n",
        "    self.final_conv = nn.Sequential(\n",
        "      block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, time):\n",
        "    # Returns the noise prediction from the noisy image x at time t\n",
        "    # Inputs:\n",
        "    #   x: noisy image tensor of size (batch_size, 3, 32, 32)\n",
        "    #   t: time-step tensor of size (batch_size,)\n",
        "    #   x[i] contains image i which has been added noise amount corresponding to t[i]\n",
        "    # Returns:\n",
        "    #   noise_pred: noise prediction made from the model, size (batch_size, 3, 32, 32)\n",
        "\n",
        "    x = self.init_conv(x)\n",
        "\n",
        "    t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "    h = []\n",
        "\n",
        "    # downsample\n",
        "    for block1, block2, attn, downsample in self.downs:\n",
        "      x = block1(x, t)\n",
        "      x = block2(x, t)\n",
        "      x = attn(x)\n",
        "      h.append(x)\n",
        "      x = downsample(x)\n",
        "\n",
        "    # bottleneck\n",
        "    x = self.mid_block1(x, t)\n",
        "    x = self.mid_attn(x)\n",
        "    x = self.mid_block2(x, t)\n",
        "\n",
        "    # upsample\n",
        "    for block1, block2, attn, upsample in self.ups:\n",
        "      x = torch.cat((x, h.pop()), dim=1)\n",
        "      x = block1(x, t)\n",
        "      x = block2(x, t)\n",
        "      x = attn(x)\n",
        "      x = upsample(x)\n",
        "\n",
        "    noise_pred = self.final_conv(x)\n",
        "    return noise_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30368b2",
      "metadata": {
        "id": "a30368b2"
      },
      "source": [
        "We define a helper function *extract* which takes as input a tensor *a* and an index tesor *t* and returns another tensor where the $i^{th}$ element of this new tensor corresponds to $a[t[i]]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B_k8rLDFzJ1C",
      "metadata": {
        "id": "B_k8rLDFzJ1C"
      },
      "outputs": [],
      "source": [
        "def extract(a, t, x_shape):\n",
        "  # Takes a data tensor a and an index tensor t, and returns a new tensor\n",
        "  # whose i^th element is just a[t[i]]. Note that this will be useful when\n",
        "  # we would want to choose the alphas or betas corresponding to different\n",
        "  # indices t's in a batched manner without for loops.\n",
        "  # Inputs:\n",
        "  #   a: Tensor, generally of shape (batch_size,)\n",
        "  #   t: Tensor, generally of shape (batch_size,)\n",
        "  #   x_shape: Shape of the data, generally (batch_size, 3, 32, 32)\n",
        "  # Returns:\n",
        "  #   out: Tensor of shape (batch_size, 1, 1, 1) generally, the number of 1s are\n",
        "  #         determined by the number of dimensions in x_shape.\n",
        "  #         out[i] contains a[t[i]]\n",
        "\n",
        "  batch_size = t.shape[0]\n",
        "  out = a.gather(-1, t.cpu())\n",
        "  return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2EnfxY7ZFFAQ",
      "metadata": {
        "id": "2EnfxY7ZFFAQ"
      },
      "source": [
        "Now, we define the different coefficients that are required in the diffusion process. In particular, we need to define the following tensors, all of which are of size $(T,)$. Also note that we are using indexing starting from 1 here, in the code all the variables with $t=1$ are set at position 0. Your task is to compute\n",
        "\n",
        "- betas: Contains $\\beta_t$ from the linear scheduling between $\\beta_1$ and $\\beta_T$, sampled over $T$ intervals\n",
        "- alphas: Contains $\\alpha_t = 1-\\beta_t$\n",
        "- sqrt_recip_alphas: Contains $\\frac{1.}{\\sqrt{{\\alpha}_t}}$\n",
        "- alphas_cumprod: Contains $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n",
        "- sqrt_alphas_cumprod: Contains $\\sqrt{\\bar{\\alpha}_t}$\n",
        "- sqrt_one_minus_alphas_cumprod: Contains $\\sqrt{1 - \\bar{\\alpha}_t}$\n",
        "- alphas_cumprod_prev: Right shift $\\bar{\\alpha}_t$; thus contains $\\prod_{i=1}^{t-1} \\alpha_i$ with the first element as 1.\n",
        "- posterior_variance: Contains $\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$\n",
        "\n",
        "**Hint:** *For computing cumulative products and right shifting, one can refer to certain PyTorch's functions that allow you to do both in a single line*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d751df2",
      "metadata": {
        "id": "5d751df2"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import alphas_betas_sequences_helper\n",
        "\n",
        "betas, alpha, sqrt_recip_alphas, alphas_cumprod, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, alphas_cumprod_prev, posterior_variance = alphas_betas_sequences_helper(beta_start, beta_end, T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nJmq5Q0mJkBX",
      "metadata": {
        "id": "nJmq5Q0mJkBX"
      },
      "source": [
        "# Forward Diffusion Process\n",
        "\n",
        "To define the forward diffusion, we need to model the distribution $q(x_t | x_0)$ in a manner that does not go through $t = 1, ..., t-1$. In particular, as defined at the start, we can obtain the distribution as\n",
        "\n",
        "\\begin{align*}\n",
        "  q(x_t | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\n",
        "\\end{align*}\n",
        "\n",
        "Since you have already computed the required coefficients above, your task is to now complete the function q_sample which takes $(x_0, t)$ as input and returns a sample from $q(x_t | x_0)$ in a batched manner, that is, in parallel for a batch of $x_0$'s and a batch of different timesteps $t$. You can use the extract function provided to get the coefficients at the right timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3752480",
      "metadata": {
        "id": "f3752480"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import q_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82bac28",
      "metadata": {
        "id": "e82bac28"
      },
      "source": [
        "Let's test the forward diffusion process on a particular image sample. We will see that the sample progressively loses all structure and ends up close to completely random noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd64f89",
      "metadata": {
        "id": "6bd64f89"
      },
      "outputs": [],
      "source": [
        "def visualize_diffusion():\n",
        "  train_dataloader, _ = get_dataloaders()\n",
        "  batch = next(iter(train_dataloader))\n",
        "  sample = batch['pixel_values'][3].unsqueeze(0)\n",
        "  noisy_images = [sample] + [q_sample(sample, torch.tensor([100 * t + 99]), (sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod)) for t in range(10)]\n",
        "  noisy_images = (torch.cat(noisy_images, dim=0) + 1.) * 0.5\n",
        "  show_image(noisy_images.clamp(0., 1.), nrow=11)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  visualize_diffusion()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0SkxPpp-LeDO",
      "metadata": {
        "id": "0SkxPpp-LeDO"
      },
      "source": [
        "# Backward Learned Diffusion Process\n",
        "\n",
        "Now suppose you have access to the model $\\epsilon_\\theta$ in the above description of diffusion models. We know that given a noisy sample $x_t$, one can obtain a slightly denoised version of this sample through the distribution $p_\\theta(x_{t-1} | x_t)$, which in our setup is now defined as\n",
        "\n",
        "\\begin{align*}\n",
        "p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(\\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\\right), \\tilde{\\beta}_t I)\n",
        "\\end{align*}\n",
        "\n",
        "Below, the task is to complete the function p_sample that takes as input the denoising model $\\epsilon_\\theta$, a batched noisy image $x$, a batched time-step $t$ and a scalar $t\\_index$, and it has to return a sample from $p(x_{t-1} | x_t)$. In the case that $t=1$ (or in code, $t=0$), please just return the mode instead of a sample. For doing this if-condition, you can use $t\\_index$ which is just a scalar instead of its batched variant $t$.\n",
        "\n",
        "Note that all the coefficients $\\bar{\\alpha}_t$, etc. are already computed above for all $t$, so use the extract function provided to obtain them at different corresponding timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7628fb3",
      "metadata": {
        "id": "f7628fb3"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import p_sample, p_sample_loop\n",
        "\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    # Returns a sample by running the sampling loop\n",
        "    with torch.no_grad():\n",
        "        return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size), timesteps=T, (betas, sqrt_one_minus_alphas_cumprod, sqrt_recip_alphas, posterior_variance))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J83yuyoppec0",
      "metadata": {
        "id": "J83yuyoppec0"
      },
      "source": [
        "# Define the Loss\n",
        "\n",
        "Now that we have both the forward and the backward diffusion process ready, we need a training criterion. In the introduction, we already saw that the optimization objective for training is to minimize:\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{t\\sim \\mathcal{U}(1,T), x_0, \\epsilon_t} \\left[|| \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t, t) ||^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "This boils down to\n",
        "\n",
        "- Generating some gaussian noise from $\\mathcal{N}(0, I)$.\n",
        "- Getting the noisy images at time $t$ in a batched, one-shot fashion.\n",
        "- Getting the estimate of noise from the noisy images.\n",
        "- Computing the loss between the estimate of noise and the actual noise.\n",
        "\n",
        "In practice here, we will use the **huber** loss instead of the squared loss; so please implement that. Feel free to use PyTorch's criterion to get the huber loss formulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7725f6cf",
      "metadata": {
        "id": "7725f6cf"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import p_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76FreGeEyDG5",
      "metadata": {
        "id": "76FreGeEyDG5"
      },
      "source": [
        "### Random sampling of time-step\n",
        "\n",
        "Finally, randomly sample time-steps from a uniform distribution over timesteps, and return a tensor of size (batch\\_size,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ezi8ZB4Dxx_b",
      "metadata": {
        "id": "Ezi8ZB4Dxx_b"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import t_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4c0fd",
      "metadata": {
        "id": "22e4c0fd"
      },
      "source": [
        "Having defined all the ingredients for **training** and **sampling** from this model, we now define the model itself and the optimizer used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5126e21",
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=input_channels,\n",
        "    dim_mults=(1, 4, 16, 64)\n",
        "  )\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7444b0b",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "Finally, let's start training!\n",
        "Visualization of the samples generated, the original dataset and the reconstructions are saved locally in the notebook! Your task is to just provide sampling of time-steps t, which should be a tensor of size (batch\\_size,) sampled uniformly from $[0, T-1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b12ed1",
      "metadata": {
        "id": "92b12ed1"
      },
      "outputs": [],
      "source": [
        "epochs = 25\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_dataloader, test_dataloader = get_dataloaders()\n",
        "  for epoch in range(epochs):\n",
        "    with tqdm(train_dataloader, unit=\"batch\", leave=False) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch: {epoch}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = batch[\"pixel_values\"].shape[0]\n",
        "        x = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        t = t_sample(T, batch_size, x.device) # Randomly sample timesteps uniformly from [0, T-1]\n",
        "\n",
        "        loss = p_losses(model, x, t, (sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Sample and Save Generated Images\n",
        "    save_image((x + 1.) * 0.5, './results_diffusion/orig.png')\n",
        "    samples = sample(model, image_size=image_size, batch_size=64, channels=input_channels)\n",
        "    samples = (torch.Tensor(samples[-1]) + 1.) * 0.5\n",
        "    save_image(samples, f'./results_diffusion/samples_{epoch}.png')\n",
        "\n",
        "  show_image(samples)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
