{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9VX-OHxC1FM"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qFHMMDtSwuW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351404be-454f-4c79-aee4-2ba20cee389a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradescope-utils\n",
        "!pip install tqdm\n",
        "!pip install GPUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvBMG-WamOAi",
        "outputId": "b9483405-301f-4cfc-f21c-220df9ca560c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradescope-utils in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7395 sha256=0b9c882d5fad0c8b233a45d497625c62edb5b09ef744365568bbe8fbdd096c9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > ParamÃ¨tres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import urllib.request\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from lstm_solution import LSTM\n",
        "from gpt1_solution import MiniGPT1\n",
        "from utils.wikitext2 import Wikitext2\n",
        "from utils.torch_utils import seed_experiment, to_device\n",
        "from utils.data_utils import save_logs\n",
        "from run_exp import train, evaluate\n",
        "import GPUtil\n",
        "\n",
        "EMBEDDINGS_URL = \"https://www.dropbox.com/s/g91502hubcmb4ob/embeddings.npz?dl=0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZr3Fh-qaGAZ"
      },
      "source": [
        "## Public tests\n",
        "Run the following cell in order to run the public tests to check to tensor shapes of the outputs of your functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRwCZpSaaE9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c744ad5-560d-4f32-8ee7-9ea5373d8104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..........\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.676s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest discover -s /content/assignment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PtvL_yKp3PW"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWiJme7XaLiR"
      },
      "source": [
        "Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularization, adding logs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUrqebfCobD1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Arguments:\n",
        "  # Data\n",
        "  data_folder: str = '/content/assignment/data'\n",
        "  batch_size: int = 16\n",
        "\n",
        "  # Model\n",
        "  model: str = 'lstm'  # [lstm, gpt1]\n",
        "  embeddings: str = '/content/assignment/data/embeddings.npz'\n",
        "  layers: int = 1\n",
        "\n",
        "  # Optimization\n",
        "  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n",
        "  epochs: int = 10\n",
        "  lr: float = 1e-3\n",
        "  momentum: float = 0.9\n",
        "  weight_decay: float = 5e-4\n",
        "\n",
        "  # Experiment\n",
        "  exp_id: str = 'debug'\n",
        "  log: bool = True\n",
        "  log_dir: str = '/content/assignment/logs'\n",
        "  seed: int = 42\n",
        "\n",
        "  # Miscellaneous\n",
        "  num_workers: int = 2\n",
        "  device: str = 'cuda'\n",
        "  progress_bar: bool = False\n",
        "  print_every: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ntfY6yyad_F"
      },
      "source": [
        "The 12 configurations you need to run in Problem 3. Be careful that there is no discrepency between the configurations defined in `run_exp.py` and the ones below. In case there is a difference, the version from `run_exp.py` should be considered the ones to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q6AwUVDX78-"
      },
      "outputs": [],
      "source": [
        "# Note: if there is any discrepency with the configurations in run_exp.py, the\n",
        "# version from run_exp.py should be the ones to use in Problem 3.\n",
        "configs = {\n",
        "  1: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id=\"lstm_layer_1_btch_16_adam\"),\n",
        "  2: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_1_btch_16_adamw\"),\n",
        "  3: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id=\"lstm_layer_1_btch_16_sgd\"),\n",
        "  4: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id=\"lstm_layer_1_btch_16_momentum\"),\n",
        "\n",
        "  5: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id=\"gpt1_layer_1_btch_16_adam\"),\n",
        "  6: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_1_btch_16_adamw\"),\n",
        "  7: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id=\"gp1_layer_1_btch_16_sgd\"),\n",
        "  8: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id=\"gpt1_layer_1_btch_16_momentum\"),\n",
        "\n",
        "  9: Arguments(model='lstm', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_2_btch_16_adamw\"),\n",
        "  10: Arguments(model='lstm', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_4_btch_16_adamw\"),\n",
        "  11: Arguments(model='gpt1', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_2_btch_16_adamw\"),\n",
        "  12: Arguments(model='gpt1', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_4_btch_16_adamw\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_gpu_usage(file_path, exp_id, note=\"\"):\n",
        "    gpu_loads, gpu_free_memory, gpu_used_memory, gpu_total_memory = GPUtil.getGPUs()[0].load, GPUtil.getGPUs()[0].memoryFree, GPUtil.getGPUs()[0].memoryUsed, GPUtil.getGPUs()[0].memoryTotal\n",
        "    gpu_usage = f\"Load: {gpu_loads}, Free Memory: {gpu_free_memory}, Used Memory: {gpu_used_memory}, Total Memory: {gpu_total_memory}\"\n",
        "    with open(file_path, 'a') as file:\n",
        "        file.write(f\"{exp_id}, {note}, {gpu_usage}\\n\")"
      ],
      "metadata": {
        "id": "thx4jllqnBp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2rjoY-5phTY"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "  # Seed the experiment, for repeatability\n",
        "  seed_experiment(args.seed)\n",
        "\n",
        "  # Dataloaders\n",
        "  train_dataset = Wikitext2(args.data_folder, split=\"train\")\n",
        "  train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  valid_dataset = Wikitext2(args.data_folder, split=\"validation\")\n",
        "  valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  test_dataset = Wikitext2(args.data_folder, split=\"test\")\n",
        "  test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  # Download the embeddings\n",
        "  if not os.path.isfile(args.embeddings):\n",
        "    print(\"Downloading embeddings...\")\n",
        "    urllib.request.urlretrieve(EMBEDDINGS_URL, args.embeddings)\n",
        "\n",
        "  # Model\n",
        "  if args.model == \"lstm\":\n",
        "    model = LSTM.load_embeddings_from(\n",
        "      args.embeddings, hidden_size=512, num_layers=args.layers\n",
        "    )\n",
        "  elif args.model == \"gpt1\":\n",
        "    model = MiniGPT1.load_embeddings_from(\n",
        "      args.embeddings, num_layers=args.layers\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(\"Unknown model {0}\".format(args.model))\n",
        "  model.to(args.device)\n",
        "\n",
        "  # Optimizer\n",
        "  if args.optimizer == \"adamw\":\n",
        "    optimizer = optim.AdamW(\n",
        "      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "  elif args.optimizer == \"adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "  elif args.optimizer == \"sgd\":\n",
        "    optimizer = optim.SGD(\n",
        "      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "  elif args.optimizer == \"momentum\":\n",
        "    optimizer = optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=args.lr,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.weight_decay,\n",
        "    )\n",
        "\n",
        "  print(\n",
        "    f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n",
        "    f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n",
        "  )\n",
        "  file_path = \"gdrive/MyDrive/Assignment2/Log/GPUUSAGELOGCF3.txt\"\n",
        "  log_gpu_usage(file_path, args.exp_id, \"Before Training\")\n",
        "  train_losses, valid_losses = [], []\n",
        "  train_ppls, valid_ppls = [], []\n",
        "  train_times, valid_times = [], []\n",
        "  for epoch in range(args.epochs):\n",
        "\n",
        "    tqdm.write(f\"====== Epoch {epoch} ======>\")\n",
        "\n",
        "    loss, ppl, wall_time = train(epoch, model, train_dataloader, optimizer, args)\n",
        "    train_losses.append(loss)\n",
        "    train_ppls.append(ppl)\n",
        "    train_times.append(wall_time)\n",
        "\n",
        "    loss, ppl, wall_time = evaluate(epoch, model, valid_dataloader, args)\n",
        "    valid_losses.append(loss)\n",
        "    valid_ppls.append(ppl)\n",
        "    valid_times.append(wall_time)\n",
        "    log_gpu_usage(file_path, args.exp_id, f\"After Epoch {epoch}\")\n",
        "  test_loss, test_ppl, test_time = evaluate(\n",
        "    epoch, model, test_dataloader, args, mode=\"test\"\n",
        "  )\n",
        "\n",
        "  print(f\"===== Best validation perplexity: {min(valid_ppls):.3f} =====>\")\n",
        "  log_gpu_usage(file_path, args.exp_id, \"After Training\")\n",
        "  return (\n",
        "    train_losses,\n",
        "    train_ppls,\n",
        "    train_times,\n",
        "    valid_losses,\n",
        "    valid_ppls,\n",
        "    valid_times,\n",
        "    test_loss,\n",
        "    test_ppl,\n",
        "    test_time,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJPWO1ppcTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd800c09-8d05-4123-bc25-d57bf867945c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized LSTM model with 34107392 total parameters, of which 3019520 are learnable.\n",
            "====== Epoch 0 ======>\n",
            "[TRAIN] Epoch: 0, Iter: 0, Loss: 10.60759\n",
            "[TRAIN] Epoch: 0, Iter: 10, Loss: 10.60043\n",
            "[TRAIN] Epoch: 0, Iter: 20, Loss: 10.59470\n",
            "[TRAIN] Epoch: 0, Iter: 30, Loss: 10.58981\n",
            "[TRAIN] Epoch: 0, Iter: 40, Loss: 10.58366\n",
            "[TRAIN] Epoch: 0, Iter: 50, Loss: 10.57872\n",
            "[TRAIN] Epoch: 0, Iter: 60, Loss: 10.57168\n",
            "[TRAIN] Epoch: 0, Iter: 70, Loss: 10.56722\n",
            "[TRAIN] Epoch: 0, Iter: 80, Loss: 10.56236\n",
            "[TRAIN] Epoch: 0, Iter: 90, Loss: 10.55735\n",
            "[TRAIN] Epoch: 0, Iter: 100, Loss: 10.55134\n",
            "[TRAIN] Epoch: 0, Iter: 110, Loss: 10.54614\n",
            "[TRAIN] Epoch: 0, Iter: 120, Loss: 10.54164\n",
            "[TRAIN] Epoch: 0, Iter: 130, Loss: 10.53424\n",
            "[TRAIN] Epoch: 0, Iter: 140, Loss: 10.52969\n",
            "[TRAIN] Epoch: 0, Iter: 150, Loss: 10.52247\n",
            "[TRAIN] Epoch: 0, Iter: 160, Loss: 10.52107\n",
            "[TRAIN] Epoch: 0, Iter: 170, Loss: 10.51572\n",
            "[TRAIN] Epoch: 0, Iter: 180, Loss: 10.50896\n",
            "[TRAIN] Epoch: 0, Iter: 190, Loss: 10.50789\n",
            "[TRAIN] Epoch: 0, Iter: 200, Loss: 10.49649\n",
            "[TRAIN] Epoch: 0, Iter: 210, Loss: 10.49200\n",
            "[TRAIN] Epoch: 0, Iter: 220, Loss: 10.48625\n",
            "[TRAIN] Epoch: 0, Iter: 230, Loss: 10.48369\n",
            "[TRAIN] Epoch: 0, Iter: 240, Loss: 10.48146\n",
            "[TRAIN] Epoch: 0, Iter: 250, Loss: 10.47567\n",
            "[TRAIN] Epoch: 0, Iter: 260, Loss: 10.47157\n",
            "[TRAIN] Epoch: 0, Iter: 270, Loss: 10.46346\n",
            "[TRAIN] Epoch: 0, Iter: 280, Loss: 10.45684\n",
            "[TRAIN] Epoch: 0, Iter: 290, Loss: 10.45504\n",
            "[TRAIN] Epoch: 0, Iter: 300, Loss: 10.45262\n",
            "[TRAIN] Epoch: 0, Iter: 310, Loss: 10.44799\n",
            "[TRAIN] Epoch: 0, Iter: 320, Loss: 10.43840\n",
            "[TRAIN] Epoch: 0, Iter: 330, Loss: 10.43467\n",
            "[TRAIN] Epoch: 0, Iter: 340, Loss: 10.43678\n",
            "[TRAIN] Epoch: 0, Iter: 350, Loss: 10.43383\n",
            "[TRAIN] Epoch: 0, Iter: 360, Loss: 10.42653\n",
            "[TRAIN] Epoch: 0, Iter: 370, Loss: 10.42288\n",
            "[TRAIN] Epoch: 0, Iter: 380, Loss: 10.41383\n",
            "[TRAIN] Epoch: 0, Iter: 390, Loss: 10.41388\n",
            "[TRAIN] Epoch: 0, Iter: 400, Loss: 10.40660\n",
            "[TRAIN] Epoch: 0, Iter: 410, Loss: 10.40666\n",
            "[TRAIN] Epoch: 0, Iter: 420, Loss: 10.40193\n",
            "[TRAIN] Epoch: 0, Iter: 430, Loss: 10.40083\n",
            "[TRAIN] Epoch: 0, Iter: 440, Loss: 10.38297\n",
            "[TRAIN] Epoch: 0, Iter: 450, Loss: 10.38940\n",
            "[TRAIN] Epoch: 0, Iter: 460, Loss: 10.38165\n",
            "[TRAIN] Epoch: 0, Iter: 470, Loss: 10.37667\n",
            "[TRAIN] Epoch: 0, Iter: 480, Loss: 10.37427\n",
            "[TRAIN] Epoch: 0, Iter: 490, Loss: 10.37720\n",
            "[TRAIN] Epoch: 0, Iter: 500, Loss: 10.36808\n",
            "[TRAIN] Epoch: 0, Iter: 510, Loss: 10.36042\n",
            "[TRAIN] Epoch: 0, Iter: 520, Loss: 10.36115\n",
            "[TRAIN] Epoch: 0, Iter: 530, Loss: 10.35410\n",
            "[TRAIN] Epoch: 0, Iter: 540, Loss: 10.35259\n",
            "[TRAIN] Epoch: 0, Iter: 550, Loss: 10.35093\n",
            "[TRAIN] Epoch: 0, Iter: 560, Loss: 10.34918\n",
            "[TRAIN] Epoch: 0, Iter: 570, Loss: 10.33789\n",
            "== [TRAIN] Epoch: 0, Perplexity: 34523.884 ==>\n",
            "[VAL] Epoch: 0, Iter: 0, Loss: 10.33395\n",
            "[VAL] Epoch: 0, Iter: 10, Loss: 10.34139\n",
            "[VAL] Epoch: 0, Iter: 20, Loss: 10.34013\n",
            "[VAL] Epoch: 0, Iter: 30, Loss: 10.33068\n",
            "[VAL] Epoch: 0, Iter: 40, Loss: 10.32784\n",
            "[VAL] Epoch: 0, Iter: 50, Loss: 10.32822\n",
            "=== [VAL] Epoch: 0, Iter: 59, Perplexity: 29825.120 ===>\n",
            "====== Epoch 1 ======>\n",
            "[TRAIN] Epoch: 1, Iter: 0, Loss: 10.33237\n",
            "[TRAIN] Epoch: 1, Iter: 10, Loss: 10.32473\n",
            "[TRAIN] Epoch: 1, Iter: 20, Loss: 10.32573\n",
            "[TRAIN] Epoch: 1, Iter: 30, Loss: 10.32011\n",
            "[TRAIN] Epoch: 1, Iter: 40, Loss: 10.31611\n",
            "[TRAIN] Epoch: 1, Iter: 50, Loss: 10.31273\n",
            "[TRAIN] Epoch: 1, Iter: 60, Loss: 10.31446\n",
            "[TRAIN] Epoch: 1, Iter: 70, Loss: 10.29999\n",
            "[TRAIN] Epoch: 1, Iter: 80, Loss: 10.29976\n",
            "[TRAIN] Epoch: 1, Iter: 90, Loss: 10.30175\n",
            "[TRAIN] Epoch: 1, Iter: 100, Loss: 10.29525\n",
            "[TRAIN] Epoch: 1, Iter: 110, Loss: 10.28391\n",
            "[TRAIN] Epoch: 1, Iter: 120, Loss: 10.29086\n",
            "[TRAIN] Epoch: 1, Iter: 130, Loss: 10.27556\n",
            "[TRAIN] Epoch: 1, Iter: 140, Loss: 10.27553\n",
            "[TRAIN] Epoch: 1, Iter: 150, Loss: 10.26862\n",
            "[TRAIN] Epoch: 1, Iter: 160, Loss: 10.26536\n",
            "[TRAIN] Epoch: 1, Iter: 170, Loss: 10.26673\n",
            "[TRAIN] Epoch: 1, Iter: 180, Loss: 10.25606\n",
            "[TRAIN] Epoch: 1, Iter: 190, Loss: 10.25710\n",
            "[TRAIN] Epoch: 1, Iter: 200, Loss: 10.25987\n",
            "[TRAIN] Epoch: 1, Iter: 210, Loss: 10.24274\n",
            "[TRAIN] Epoch: 1, Iter: 220, Loss: 10.23798\n",
            "[TRAIN] Epoch: 1, Iter: 230, Loss: 10.24914\n",
            "[TRAIN] Epoch: 1, Iter: 240, Loss: 10.23176\n",
            "[TRAIN] Epoch: 1, Iter: 250, Loss: 10.22394\n",
            "[TRAIN] Epoch: 1, Iter: 260, Loss: 10.22768\n",
            "[TRAIN] Epoch: 1, Iter: 270, Loss: 10.21902\n",
            "[TRAIN] Epoch: 1, Iter: 280, Loss: 10.21313\n",
            "[TRAIN] Epoch: 1, Iter: 290, Loss: 10.21188\n",
            "[TRAIN] Epoch: 1, Iter: 300, Loss: 10.21865\n",
            "[TRAIN] Epoch: 1, Iter: 310, Loss: 10.21647\n",
            "[TRAIN] Epoch: 1, Iter: 320, Loss: 10.20477\n",
            "[TRAIN] Epoch: 1, Iter: 330, Loss: 10.18986\n",
            "[TRAIN] Epoch: 1, Iter: 340, Loss: 10.19267\n",
            "[TRAIN] Epoch: 1, Iter: 350, Loss: 10.17624\n",
            "[TRAIN] Epoch: 1, Iter: 360, Loss: 10.17709\n",
            "[TRAIN] Epoch: 1, Iter: 370, Loss: 10.18780\n",
            "[TRAIN] Epoch: 1, Iter: 380, Loss: 10.17430\n",
            "[TRAIN] Epoch: 1, Iter: 390, Loss: 10.16551\n",
            "[TRAIN] Epoch: 1, Iter: 400, Loss: 10.16923\n",
            "[TRAIN] Epoch: 1, Iter: 410, Loss: 10.17819\n",
            "[TRAIN] Epoch: 1, Iter: 420, Loss: 10.15980\n",
            "[TRAIN] Epoch: 1, Iter: 430, Loss: 10.14499\n",
            "[TRAIN] Epoch: 1, Iter: 440, Loss: 10.13598\n",
            "[TRAIN] Epoch: 1, Iter: 450, Loss: 10.16182\n",
            "[TRAIN] Epoch: 1, Iter: 460, Loss: 10.14857\n",
            "[TRAIN] Epoch: 1, Iter: 470, Loss: 10.13940\n",
            "[TRAIN] Epoch: 1, Iter: 480, Loss: 10.12077\n",
            "[TRAIN] Epoch: 1, Iter: 490, Loss: 10.12209\n",
            "[TRAIN] Epoch: 1, Iter: 500, Loss: 10.11188\n",
            "[TRAIN] Epoch: 1, Iter: 510, Loss: 10.12881\n",
            "[TRAIN] Epoch: 1, Iter: 520, Loss: 10.10260\n",
            "[TRAIN] Epoch: 1, Iter: 530, Loss: 10.11830\n",
            "[TRAIN] Epoch: 1, Iter: 540, Loss: 10.10858\n",
            "[TRAIN] Epoch: 1, Iter: 550, Loss: 10.10392\n",
            "[TRAIN] Epoch: 1, Iter: 560, Loss: 10.11009\n",
            "[TRAIN] Epoch: 1, Iter: 570, Loss: 10.09429\n",
            "== [TRAIN] Epoch: 1, Perplexity: 26931.167 ==>\n",
            "[VAL] Epoch: 1, Iter: 0, Loss: 10.09245\n",
            "[VAL] Epoch: 1, Iter: 10, Loss: 10.10623\n",
            "[VAL] Epoch: 1, Iter: 20, Loss: 10.10057\n",
            "[VAL] Epoch: 1, Iter: 30, Loss: 10.08397\n",
            "[VAL] Epoch: 1, Iter: 40, Loss: 10.07780\n",
            "[VAL] Epoch: 1, Iter: 50, Loss: 10.07886\n",
            "=== [VAL] Epoch: 1, Iter: 59, Perplexity: 23369.295 ===>\n",
            "====== Epoch 2 ======>\n",
            "[TRAIN] Epoch: 2, Iter: 0, Loss: 10.08741\n",
            "[TRAIN] Epoch: 2, Iter: 10, Loss: 10.06995\n",
            "[TRAIN] Epoch: 2, Iter: 20, Loss: 10.07992\n",
            "[TRAIN] Epoch: 2, Iter: 30, Loss: 10.06597\n",
            "[TRAIN] Epoch: 2, Iter: 40, Loss: 10.06583\n",
            "[TRAIN] Epoch: 2, Iter: 50, Loss: 10.05710\n",
            "[TRAIN] Epoch: 2, Iter: 60, Loss: 10.05784\n",
            "[TRAIN] Epoch: 2, Iter: 70, Loss: 10.04599\n",
            "[TRAIN] Epoch: 2, Iter: 80, Loss: 10.05802\n",
            "[TRAIN] Epoch: 2, Iter: 90, Loss: 10.06098\n",
            "[TRAIN] Epoch: 2, Iter: 100, Loss: 10.03515\n",
            "[TRAIN] Epoch: 2, Iter: 110, Loss: 10.03908\n",
            "[TRAIN] Epoch: 2, Iter: 120, Loss: 10.03356\n",
            "[TRAIN] Epoch: 2, Iter: 130, Loss: 10.03539\n",
            "[TRAIN] Epoch: 2, Iter: 140, Loss: 10.03794\n",
            "[TRAIN] Epoch: 2, Iter: 150, Loss: 10.00946\n",
            "[TRAIN] Epoch: 2, Iter: 160, Loss: 10.01405\n",
            "[TRAIN] Epoch: 2, Iter: 170, Loss: 10.00029\n",
            "[TRAIN] Epoch: 2, Iter: 180, Loss: 10.01321\n",
            "[TRAIN] Epoch: 2, Iter: 190, Loss: 10.00666\n",
            "[TRAIN] Epoch: 2, Iter: 200, Loss: 9.99611\n",
            "[TRAIN] Epoch: 2, Iter: 210, Loss: 9.98025\n",
            "[TRAIN] Epoch: 2, Iter: 220, Loss: 9.99220\n",
            "[TRAIN] Epoch: 2, Iter: 230, Loss: 9.97813\n",
            "[TRAIN] Epoch: 2, Iter: 240, Loss: 9.95773\n",
            "[TRAIN] Epoch: 2, Iter: 250, Loss: 9.97155\n",
            "[TRAIN] Epoch: 2, Iter: 260, Loss: 9.98174\n",
            "[TRAIN] Epoch: 2, Iter: 270, Loss: 9.98940\n",
            "[TRAIN] Epoch: 2, Iter: 280, Loss: 9.95541\n",
            "[TRAIN] Epoch: 2, Iter: 290, Loss: 9.94457\n",
            "[TRAIN] Epoch: 2, Iter: 300, Loss: 9.95745\n",
            "[TRAIN] Epoch: 2, Iter: 310, Loss: 9.92001\n",
            "[TRAIN] Epoch: 2, Iter: 320, Loss: 9.92759\n",
            "[TRAIN] Epoch: 2, Iter: 330, Loss: 9.93872\n",
            "[TRAIN] Epoch: 2, Iter: 340, Loss: 9.92373\n",
            "[TRAIN] Epoch: 2, Iter: 350, Loss: 9.90968\n",
            "[TRAIN] Epoch: 2, Iter: 360, Loss: 9.92602\n",
            "[TRAIN] Epoch: 2, Iter: 370, Loss: 9.91190\n",
            "[TRAIN] Epoch: 2, Iter: 380, Loss: 9.89796\n",
            "[TRAIN] Epoch: 2, Iter: 390, Loss: 9.91076\n",
            "[TRAIN] Epoch: 2, Iter: 400, Loss: 9.89359\n",
            "[TRAIN] Epoch: 2, Iter: 410, Loss: 9.90946\n",
            "[TRAIN] Epoch: 2, Iter: 420, Loss: 9.88509\n",
            "[TRAIN] Epoch: 2, Iter: 430, Loss: 9.90087\n",
            "[TRAIN] Epoch: 2, Iter: 440, Loss: 9.86079\n",
            "[TRAIN] Epoch: 2, Iter: 450, Loss: 9.84266\n",
            "[TRAIN] Epoch: 2, Iter: 460, Loss: 9.85944\n",
            "[TRAIN] Epoch: 2, Iter: 470, Loss: 9.87201\n",
            "[TRAIN] Epoch: 2, Iter: 480, Loss: 9.85922\n",
            "[TRAIN] Epoch: 2, Iter: 490, Loss: 9.83272\n",
            "[TRAIN] Epoch: 2, Iter: 500, Loss: 9.84045\n",
            "[TRAIN] Epoch: 2, Iter: 510, Loss: 9.83264\n",
            "[TRAIN] Epoch: 2, Iter: 520, Loss: 9.83258\n",
            "[TRAIN] Epoch: 2, Iter: 530, Loss: 9.84922\n",
            "[TRAIN] Epoch: 2, Iter: 540, Loss: 9.79999\n",
            "[TRAIN] Epoch: 2, Iter: 550, Loss: 9.81715\n",
            "[TRAIN] Epoch: 2, Iter: 560, Loss: 9.80565\n",
            "[TRAIN] Epoch: 2, Iter: 570, Loss: 9.80739\n",
            "== [TRAIN] Epoch: 2, Perplexity: 20691.752 ==>\n",
            "[VAL] Epoch: 2, Iter: 0, Loss: 9.80094\n",
            "[VAL] Epoch: 2, Iter: 10, Loss: 9.82279\n",
            "[VAL] Epoch: 2, Iter: 20, Loss: 9.81253\n",
            "[VAL] Epoch: 2, Iter: 30, Loss: 9.78536\n",
            "[VAL] Epoch: 2, Iter: 40, Loss: 9.77549\n",
            "[VAL] Epoch: 2, Iter: 50, Loss: 9.77701\n",
            "=== [VAL] Epoch: 2, Iter: 59, Perplexity: 17404.281 ===>\n",
            "====== Epoch 3 ======>\n",
            "[TRAIN] Epoch: 3, Iter: 0, Loss: 9.80718\n",
            "[TRAIN] Epoch: 3, Iter: 10, Loss: 9.77368\n",
            "[TRAIN] Epoch: 3, Iter: 20, Loss: 9.78400\n",
            "[TRAIN] Epoch: 3, Iter: 30, Loss: 9.77600\n",
            "[TRAIN] Epoch: 3, Iter: 40, Loss: 9.76461\n",
            "[TRAIN] Epoch: 3, Iter: 50, Loss: 9.75858\n",
            "[TRAIN] Epoch: 3, Iter: 60, Loss: 9.75247\n",
            "[TRAIN] Epoch: 3, Iter: 70, Loss: 9.73787\n",
            "[TRAIN] Epoch: 3, Iter: 80, Loss: 9.75770\n",
            "[TRAIN] Epoch: 3, Iter: 90, Loss: 9.74403\n",
            "[TRAIN] Epoch: 3, Iter: 100, Loss: 9.73710\n",
            "[TRAIN] Epoch: 3, Iter: 110, Loss: 9.72686\n",
            "[TRAIN] Epoch: 3, Iter: 120, Loss: 9.72756\n",
            "[TRAIN] Epoch: 3, Iter: 130, Loss: 9.71622\n",
            "[TRAIN] Epoch: 3, Iter: 140, Loss: 9.71006\n",
            "[TRAIN] Epoch: 3, Iter: 150, Loss: 9.71507\n",
            "[TRAIN] Epoch: 3, Iter: 160, Loss: 9.69716\n",
            "[TRAIN] Epoch: 3, Iter: 170, Loss: 9.67495\n",
            "[TRAIN] Epoch: 3, Iter: 180, Loss: 9.71003\n",
            "[TRAIN] Epoch: 3, Iter: 190, Loss: 9.68042\n",
            "[TRAIN] Epoch: 3, Iter: 200, Loss: 9.67395\n",
            "[TRAIN] Epoch: 3, Iter: 210, Loss: 9.67623\n",
            "[TRAIN] Epoch: 3, Iter: 220, Loss: 9.63913\n",
            "[TRAIN] Epoch: 3, Iter: 230, Loss: 9.63808\n",
            "[TRAIN] Epoch: 3, Iter: 240, Loss: 9.66956\n",
            "[TRAIN] Epoch: 3, Iter: 250, Loss: 9.64482\n",
            "[TRAIN] Epoch: 3, Iter: 260, Loss: 9.58694\n",
            "[TRAIN] Epoch: 3, Iter: 270, Loss: 9.61644\n",
            "[TRAIN] Epoch: 3, Iter: 280, Loss: 9.62576\n",
            "[TRAIN] Epoch: 3, Iter: 290, Loss: 9.59310\n",
            "[TRAIN] Epoch: 3, Iter: 300, Loss: 9.57695\n",
            "[TRAIN] Epoch: 3, Iter: 310, Loss: 9.58422\n",
            "[TRAIN] Epoch: 3, Iter: 320, Loss: 9.58356\n",
            "[TRAIN] Epoch: 3, Iter: 330, Loss: 9.59377\n",
            "[TRAIN] Epoch: 3, Iter: 340, Loss: 9.58228\n",
            "[TRAIN] Epoch: 3, Iter: 350, Loss: 9.55983\n",
            "[TRAIN] Epoch: 3, Iter: 360, Loss: 9.57130\n",
            "[TRAIN] Epoch: 3, Iter: 370, Loss: 9.53094\n",
            "[TRAIN] Epoch: 3, Iter: 380, Loss: 9.55682\n",
            "[TRAIN] Epoch: 3, Iter: 390, Loss: 9.48978\n",
            "[TRAIN] Epoch: 3, Iter: 400, Loss: 9.49684\n",
            "[TRAIN] Epoch: 3, Iter: 410, Loss: 9.52061\n",
            "[TRAIN] Epoch: 3, Iter: 420, Loss: 9.47772\n",
            "[TRAIN] Epoch: 3, Iter: 430, Loss: 9.57014\n",
            "[TRAIN] Epoch: 3, Iter: 440, Loss: 9.49307\n",
            "[TRAIN] Epoch: 3, Iter: 450, Loss: 9.46236\n",
            "[TRAIN] Epoch: 3, Iter: 460, Loss: 9.45935\n",
            "[TRAIN] Epoch: 3, Iter: 470, Loss: 9.44194\n",
            "[TRAIN] Epoch: 3, Iter: 480, Loss: 9.45960\n",
            "[TRAIN] Epoch: 3, Iter: 490, Loss: 9.43042\n",
            "[TRAIN] Epoch: 3, Iter: 500, Loss: 9.41180\n",
            "[TRAIN] Epoch: 3, Iter: 510, Loss: 9.44222\n",
            "[TRAIN] Epoch: 3, Iter: 520, Loss: 9.42293\n",
            "[TRAIN] Epoch: 3, Iter: 530, Loss: 9.41751\n",
            "[TRAIN] Epoch: 3, Iter: 540, Loss: 9.41369\n",
            "[TRAIN] Epoch: 3, Iter: 550, Loss: 9.41526\n",
            "[TRAIN] Epoch: 3, Iter: 560, Loss: 9.38237\n",
            "[TRAIN] Epoch: 3, Iter: 570, Loss: 9.37448\n",
            "== [TRAIN] Epoch: 3, Perplexity: 14553.011 ==>\n",
            "[VAL] Epoch: 3, Iter: 0, Loss: 9.37414\n",
            "[VAL] Epoch: 3, Iter: 10, Loss: 9.40819\n",
            "[VAL] Epoch: 3, Iter: 20, Loss: 9.39220\n",
            "[VAL] Epoch: 3, Iter: 30, Loss: 9.34763\n",
            "[VAL] Epoch: 3, Iter: 40, Loss: 9.33444\n",
            "[VAL] Epoch: 3, Iter: 50, Loss: 9.33451\n",
            "=== [VAL] Epoch: 3, Iter: 59, Perplexity: 11310.482 ===>\n",
            "====== Epoch 4 ======>\n",
            "[TRAIN] Epoch: 4, Iter: 0, Loss: 9.35456\n",
            "[TRAIN] Epoch: 4, Iter: 10, Loss: 9.34223\n",
            "[TRAIN] Epoch: 4, Iter: 20, Loss: 9.33061\n",
            "[TRAIN] Epoch: 4, Iter: 30, Loss: 9.31436\n",
            "[TRAIN] Epoch: 4, Iter: 40, Loss: 9.35182\n",
            "[TRAIN] Epoch: 4, Iter: 50, Loss: 9.31596\n",
            "[TRAIN] Epoch: 4, Iter: 60, Loss: 9.30586\n",
            "[TRAIN] Epoch: 4, Iter: 70, Loss: 9.29145\n",
            "[TRAIN] Epoch: 4, Iter: 80, Loss: 9.28860\n",
            "[TRAIN] Epoch: 4, Iter: 90, Loss: 9.27073\n",
            "[TRAIN] Epoch: 4, Iter: 100, Loss: 9.27374\n",
            "[TRAIN] Epoch: 4, Iter: 110, Loss: 9.24535\n",
            "[TRAIN] Epoch: 4, Iter: 120, Loss: 9.28681\n",
            "[TRAIN] Epoch: 4, Iter: 130, Loss: 9.21360\n",
            "[TRAIN] Epoch: 4, Iter: 140, Loss: 9.21859\n",
            "[TRAIN] Epoch: 4, Iter: 150, Loss: 9.20157\n",
            "[TRAIN] Epoch: 4, Iter: 160, Loss: 9.19667\n",
            "[TRAIN] Epoch: 4, Iter: 170, Loss: 9.19513\n",
            "[TRAIN] Epoch: 4, Iter: 180, Loss: 9.14384\n",
            "[TRAIN] Epoch: 4, Iter: 190, Loss: 9.14272\n",
            "[TRAIN] Epoch: 4, Iter: 200, Loss: 9.15073\n",
            "[TRAIN] Epoch: 4, Iter: 210, Loss: 9.10335\n",
            "[TRAIN] Epoch: 4, Iter: 220, Loss: 9.16919\n",
            "[TRAIN] Epoch: 4, Iter: 230, Loss: 9.13458\n",
            "[TRAIN] Epoch: 4, Iter: 240, Loss: 9.08613\n",
            "[TRAIN] Epoch: 4, Iter: 250, Loss: 9.12599\n",
            "[TRAIN] Epoch: 4, Iter: 260, Loss: 9.08126\n",
            "[TRAIN] Epoch: 4, Iter: 270, Loss: 9.06863\n",
            "[TRAIN] Epoch: 4, Iter: 280, Loss: 9.03911\n",
            "[TRAIN] Epoch: 4, Iter: 290, Loss: 9.04579\n",
            "[TRAIN] Epoch: 4, Iter: 300, Loss: 8.99936\n",
            "[TRAIN] Epoch: 4, Iter: 310, Loss: 9.02556\n",
            "[TRAIN] Epoch: 4, Iter: 320, Loss: 9.00503\n",
            "[TRAIN] Epoch: 4, Iter: 330, Loss: 8.97056\n",
            "[TRAIN] Epoch: 4, Iter: 340, Loss: 8.97064\n",
            "[TRAIN] Epoch: 4, Iter: 350, Loss: 8.98852\n",
            "[TRAIN] Epoch: 4, Iter: 360, Loss: 8.93861\n",
            "[TRAIN] Epoch: 4, Iter: 370, Loss: 8.91720\n",
            "[TRAIN] Epoch: 4, Iter: 380, Loss: 8.90346\n",
            "[TRAIN] Epoch: 4, Iter: 390, Loss: 8.91907\n",
            "[TRAIN] Epoch: 4, Iter: 400, Loss: 8.83097\n",
            "[TRAIN] Epoch: 4, Iter: 410, Loss: 8.88320\n",
            "[TRAIN] Epoch: 4, Iter: 420, Loss: 8.82505\n",
            "[TRAIN] Epoch: 4, Iter: 430, Loss: 8.83181\n",
            "[TRAIN] Epoch: 4, Iter: 440, Loss: 8.86095\n",
            "[TRAIN] Epoch: 4, Iter: 450, Loss: 8.87507\n",
            "[TRAIN] Epoch: 4, Iter: 460, Loss: 8.82260\n",
            "[TRAIN] Epoch: 4, Iter: 470, Loss: 8.72728\n",
            "[TRAIN] Epoch: 4, Iter: 480, Loss: 8.79530\n",
            "[TRAIN] Epoch: 4, Iter: 490, Loss: 8.80199\n",
            "[TRAIN] Epoch: 4, Iter: 500, Loss: 8.71064\n",
            "[TRAIN] Epoch: 4, Iter: 510, Loss: 8.74107\n",
            "[TRAIN] Epoch: 4, Iter: 520, Loss: 8.68963\n",
            "[TRAIN] Epoch: 4, Iter: 530, Loss: 8.63218\n",
            "[TRAIN] Epoch: 4, Iter: 540, Loss: 8.67813\n",
            "[TRAIN] Epoch: 4, Iter: 550, Loss: 8.64495\n",
            "[TRAIN] Epoch: 4, Iter: 560, Loss: 8.62557\n",
            "[TRAIN] Epoch: 4, Iter: 570, Loss: 8.70439\n",
            "== [TRAIN] Epoch: 4, Perplexity: 8200.312 ==>\n",
            "[VAL] Epoch: 4, Iter: 0, Loss: 8.63378\n",
            "[VAL] Epoch: 4, Iter: 10, Loss: 8.69481\n",
            "[VAL] Epoch: 4, Iter: 20, Loss: 8.66538\n",
            "[VAL] Epoch: 4, Iter: 30, Loss: 8.58442\n",
            "[VAL] Epoch: 4, Iter: 40, Loss: 8.56238\n",
            "[VAL] Epoch: 4, Iter: 50, Loss: 8.55937\n",
            "=== [VAL] Epoch: 4, Iter: 59, Perplexity: 5335.880 ===>\n",
            "====== Epoch 5 ======>\n",
            "[TRAIN] Epoch: 5, Iter: 0, Loss: 8.69162\n",
            "[TRAIN] Epoch: 5, Iter: 10, Loss: 8.64272\n",
            "[TRAIN] Epoch: 5, Iter: 20, Loss: 8.54184\n",
            "[TRAIN] Epoch: 5, Iter: 30, Loss: 8.63340\n",
            "[TRAIN] Epoch: 5, Iter: 40, Loss: 8.54632\n",
            "[TRAIN] Epoch: 5, Iter: 50, Loss: 8.53762\n",
            "[TRAIN] Epoch: 5, Iter: 60, Loss: 8.53844\n",
            "[TRAIN] Epoch: 5, Iter: 70, Loss: 8.49409\n",
            "[TRAIN] Epoch: 5, Iter: 80, Loss: 8.52361\n",
            "[TRAIN] Epoch: 5, Iter: 90, Loss: 8.44745\n",
            "[TRAIN] Epoch: 5, Iter: 100, Loss: 8.52645\n",
            "[TRAIN] Epoch: 5, Iter: 110, Loss: 8.40266\n",
            "[TRAIN] Epoch: 5, Iter: 120, Loss: 8.38907\n",
            "[TRAIN] Epoch: 5, Iter: 130, Loss: 8.38868\n",
            "[TRAIN] Epoch: 5, Iter: 140, Loss: 8.43651\n",
            "[TRAIN] Epoch: 5, Iter: 150, Loss: 8.43081\n",
            "[TRAIN] Epoch: 5, Iter: 160, Loss: 8.37839\n",
            "[TRAIN] Epoch: 5, Iter: 170, Loss: 8.34046\n",
            "[TRAIN] Epoch: 5, Iter: 180, Loss: 8.29974\n",
            "[TRAIN] Epoch: 5, Iter: 190, Loss: 8.26785\n",
            "[TRAIN] Epoch: 5, Iter: 200, Loss: 8.26439\n",
            "[TRAIN] Epoch: 5, Iter: 210, Loss: 8.26562\n",
            "[TRAIN] Epoch: 5, Iter: 220, Loss: 8.22292\n",
            "[TRAIN] Epoch: 5, Iter: 230, Loss: 8.30079\n",
            "[TRAIN] Epoch: 5, Iter: 240, Loss: 8.21873\n",
            "[TRAIN] Epoch: 5, Iter: 250, Loss: 8.37025\n",
            "[TRAIN] Epoch: 5, Iter: 260, Loss: 8.28462\n",
            "[TRAIN] Epoch: 5, Iter: 270, Loss: 8.26320\n",
            "[TRAIN] Epoch: 5, Iter: 280, Loss: 8.23069\n",
            "[TRAIN] Epoch: 5, Iter: 290, Loss: 8.25461\n",
            "[TRAIN] Epoch: 5, Iter: 300, Loss: 8.25732\n",
            "[TRAIN] Epoch: 5, Iter: 310, Loss: 8.20040\n",
            "[TRAIN] Epoch: 5, Iter: 320, Loss: 8.18060\n",
            "[TRAIN] Epoch: 5, Iter: 330, Loss: 8.17060\n",
            "[TRAIN] Epoch: 5, Iter: 340, Loss: 8.16645\n",
            "[TRAIN] Epoch: 5, Iter: 350, Loss: 8.09373\n",
            "[TRAIN] Epoch: 5, Iter: 360, Loss: 8.11132\n",
            "[TRAIN] Epoch: 5, Iter: 370, Loss: 8.15676\n",
            "[TRAIN] Epoch: 5, Iter: 380, Loss: 8.16634\n",
            "[TRAIN] Epoch: 5, Iter: 390, Loss: 8.16538\n",
            "[TRAIN] Epoch: 5, Iter: 400, Loss: 8.15331\n",
            "[TRAIN] Epoch: 5, Iter: 410, Loss: 8.21240\n",
            "[TRAIN] Epoch: 5, Iter: 420, Loss: 8.09438\n",
            "[TRAIN] Epoch: 5, Iter: 430, Loss: 8.18981\n",
            "[TRAIN] Epoch: 5, Iter: 440, Loss: 8.17414\n",
            "[TRAIN] Epoch: 5, Iter: 450, Loss: 8.02198\n",
            "[TRAIN] Epoch: 5, Iter: 460, Loss: 8.07912\n",
            "[TRAIN] Epoch: 5, Iter: 470, Loss: 8.01495\n",
            "[TRAIN] Epoch: 5, Iter: 480, Loss: 8.21826\n",
            "[TRAIN] Epoch: 5, Iter: 490, Loss: 8.09280\n",
            "[TRAIN] Epoch: 5, Iter: 500, Loss: 8.07047\n",
            "[TRAIN] Epoch: 5, Iter: 510, Loss: 8.07151\n",
            "[TRAIN] Epoch: 5, Iter: 520, Loss: 8.09476\n",
            "[TRAIN] Epoch: 5, Iter: 530, Loss: 8.08343\n",
            "[TRAIN] Epoch: 5, Iter: 540, Loss: 8.17265\n",
            "[TRAIN] Epoch: 5, Iter: 550, Loss: 8.09595\n",
            "[TRAIN] Epoch: 5, Iter: 560, Loss: 7.94545\n",
            "[TRAIN] Epoch: 5, Iter: 570, Loss: 8.13530\n",
            "== [TRAIN] Epoch: 5, Perplexity: 3812.497 ==>\n",
            "[VAL] Epoch: 5, Iter: 0, Loss: 8.09332\n",
            "[VAL] Epoch: 5, Iter: 10, Loss: 8.19461\n",
            "[VAL] Epoch: 5, Iter: 20, Loss: 8.14499\n",
            "[VAL] Epoch: 5, Iter: 30, Loss: 8.02155\n",
            "[VAL] Epoch: 5, Iter: 40, Loss: 7.97353\n",
            "[VAL] Epoch: 5, Iter: 50, Loss: 7.96533\n",
            "=== [VAL] Epoch: 5, Iter: 59, Perplexity: 3053.653 ===>\n",
            "====== Epoch 6 ======>\n",
            "[TRAIN] Epoch: 6, Iter: 0, Loss: 8.02609\n",
            "[TRAIN] Epoch: 6, Iter: 10, Loss: 7.99346\n",
            "[TRAIN] Epoch: 6, Iter: 20, Loss: 8.08168\n",
            "[TRAIN] Epoch: 6, Iter: 30, Loss: 8.04079\n",
            "[TRAIN] Epoch: 6, Iter: 40, Loss: 8.14400\n",
            "[TRAIN] Epoch: 6, Iter: 50, Loss: 8.03769\n",
            "[TRAIN] Epoch: 6, Iter: 60, Loss: 8.08649\n",
            "[TRAIN] Epoch: 6, Iter: 70, Loss: 8.07988\n",
            "[TRAIN] Epoch: 6, Iter: 80, Loss: 8.05522\n",
            "[TRAIN] Epoch: 6, Iter: 90, Loss: 8.03009\n",
            "[TRAIN] Epoch: 6, Iter: 100, Loss: 8.10474\n",
            "[TRAIN] Epoch: 6, Iter: 110, Loss: 8.02299\n",
            "[TRAIN] Epoch: 6, Iter: 120, Loss: 8.08523\n",
            "[TRAIN] Epoch: 6, Iter: 130, Loss: 8.05974\n",
            "[TRAIN] Epoch: 6, Iter: 140, Loss: 8.02935\n",
            "[TRAIN] Epoch: 6, Iter: 150, Loss: 7.93779\n",
            "[TRAIN] Epoch: 6, Iter: 160, Loss: 8.10035\n",
            "[TRAIN] Epoch: 6, Iter: 170, Loss: 8.00449\n",
            "[TRAIN] Epoch: 6, Iter: 180, Loss: 7.98663\n",
            "[TRAIN] Epoch: 6, Iter: 190, Loss: 7.97437\n",
            "[TRAIN] Epoch: 6, Iter: 200, Loss: 8.02923\n",
            "[TRAIN] Epoch: 6, Iter: 210, Loss: 7.99849\n",
            "[TRAIN] Epoch: 6, Iter: 220, Loss: 8.08089\n",
            "[TRAIN] Epoch: 6, Iter: 230, Loss: 8.08768\n",
            "[TRAIN] Epoch: 6, Iter: 240, Loss: 7.94654\n",
            "[TRAIN] Epoch: 6, Iter: 250, Loss: 8.13639\n",
            "[TRAIN] Epoch: 6, Iter: 260, Loss: 8.00799\n",
            "[TRAIN] Epoch: 6, Iter: 270, Loss: 8.03173\n",
            "[TRAIN] Epoch: 6, Iter: 280, Loss: 8.04954\n",
            "[TRAIN] Epoch: 6, Iter: 290, Loss: 8.05245\n",
            "[TRAIN] Epoch: 6, Iter: 300, Loss: 8.12424\n",
            "[TRAIN] Epoch: 6, Iter: 310, Loss: 8.08378\n",
            "[TRAIN] Epoch: 6, Iter: 320, Loss: 8.00980\n",
            "[TRAIN] Epoch: 6, Iter: 330, Loss: 8.08372\n",
            "[TRAIN] Epoch: 6, Iter: 340, Loss: 7.90118\n",
            "[TRAIN] Epoch: 6, Iter: 350, Loss: 7.96007\n",
            "[TRAIN] Epoch: 6, Iter: 360, Loss: 7.88543\n",
            "[TRAIN] Epoch: 6, Iter: 370, Loss: 8.04074\n",
            "[TRAIN] Epoch: 6, Iter: 380, Loss: 8.00801\n",
            "[TRAIN] Epoch: 6, Iter: 390, Loss: 7.84708\n",
            "[TRAIN] Epoch: 6, Iter: 400, Loss: 7.90479\n",
            "[TRAIN] Epoch: 6, Iter: 410, Loss: 8.04025\n",
            "[TRAIN] Epoch: 6, Iter: 420, Loss: 7.97763\n",
            "[TRAIN] Epoch: 6, Iter: 430, Loss: 7.96317\n",
            "[TRAIN] Epoch: 6, Iter: 440, Loss: 8.10959\n",
            "[TRAIN] Epoch: 6, Iter: 450, Loss: 7.91003\n",
            "[TRAIN] Epoch: 6, Iter: 460, Loss: 7.90361\n",
            "[TRAIN] Epoch: 6, Iter: 470, Loss: 8.09242\n",
            "[TRAIN] Epoch: 6, Iter: 480, Loss: 7.98651\n",
            "[TRAIN] Epoch: 6, Iter: 490, Loss: 7.82754\n",
            "[TRAIN] Epoch: 6, Iter: 500, Loss: 7.99514\n",
            "[TRAIN] Epoch: 6, Iter: 510, Loss: 7.93428\n",
            "[TRAIN] Epoch: 6, Iter: 520, Loss: 7.94659\n",
            "[TRAIN] Epoch: 6, Iter: 530, Loss: 7.83597\n",
            "[TRAIN] Epoch: 6, Iter: 540, Loss: 7.94219\n",
            "[TRAIN] Epoch: 6, Iter: 550, Loss: 7.95111\n",
            "[TRAIN] Epoch: 6, Iter: 560, Loss: 7.91647\n",
            "[TRAIN] Epoch: 6, Iter: 570, Loss: 8.08340\n",
            "== [TRAIN] Epoch: 6, Perplexity: 2953.218 ==>\n",
            "[VAL] Epoch: 6, Iter: 0, Loss: 7.98603\n",
            "[VAL] Epoch: 6, Iter: 10, Loss: 8.08944\n",
            "[VAL] Epoch: 6, Iter: 20, Loss: 8.04734\n",
            "[VAL] Epoch: 6, Iter: 30, Loss: 7.93340\n",
            "[VAL] Epoch: 6, Iter: 40, Loss: 7.85529\n",
            "[VAL] Epoch: 6, Iter: 50, Loss: 7.83367\n",
            "=== [VAL] Epoch: 6, Iter: 59, Perplexity: 2736.522 ===>\n",
            "====== Epoch 7 ======>\n",
            "[TRAIN] Epoch: 7, Iter: 0, Loss: 7.97079\n",
            "[TRAIN] Epoch: 7, Iter: 10, Loss: 7.93301\n",
            "[TRAIN] Epoch: 7, Iter: 20, Loss: 7.90228\n",
            "[TRAIN] Epoch: 7, Iter: 30, Loss: 7.93408\n",
            "[TRAIN] Epoch: 7, Iter: 40, Loss: 8.01029\n",
            "[TRAIN] Epoch: 7, Iter: 50, Loss: 8.08631\n",
            "[TRAIN] Epoch: 7, Iter: 60, Loss: 7.91044\n",
            "[TRAIN] Epoch: 7, Iter: 70, Loss: 7.99334\n",
            "[TRAIN] Epoch: 7, Iter: 80, Loss: 7.96177\n",
            "[TRAIN] Epoch: 7, Iter: 90, Loss: 7.98809\n",
            "[TRAIN] Epoch: 7, Iter: 100, Loss: 7.92531\n",
            "[TRAIN] Epoch: 7, Iter: 110, Loss: 7.98477\n",
            "[TRAIN] Epoch: 7, Iter: 120, Loss: 7.89002\n",
            "[TRAIN] Epoch: 7, Iter: 130, Loss: 7.86497\n",
            "[TRAIN] Epoch: 7, Iter: 140, Loss: 7.96228\n",
            "[TRAIN] Epoch: 7, Iter: 150, Loss: 7.89874\n",
            "[TRAIN] Epoch: 7, Iter: 160, Loss: 7.95414\n",
            "[TRAIN] Epoch: 7, Iter: 170, Loss: 7.88259\n",
            "[TRAIN] Epoch: 7, Iter: 180, Loss: 8.10782\n",
            "[TRAIN] Epoch: 7, Iter: 190, Loss: 7.90957\n",
            "[TRAIN] Epoch: 7, Iter: 200, Loss: 7.90103\n",
            "[TRAIN] Epoch: 7, Iter: 210, Loss: 7.89065\n",
            "[TRAIN] Epoch: 7, Iter: 220, Loss: 8.10077\n",
            "[TRAIN] Epoch: 7, Iter: 230, Loss: 7.93364\n",
            "[TRAIN] Epoch: 7, Iter: 240, Loss: 7.76745\n",
            "[TRAIN] Epoch: 7, Iter: 250, Loss: 7.87785\n",
            "[TRAIN] Epoch: 7, Iter: 260, Loss: 7.87084\n",
            "[TRAIN] Epoch: 7, Iter: 270, Loss: 7.99889\n",
            "[TRAIN] Epoch: 7, Iter: 280, Loss: 7.82602\n",
            "[TRAIN] Epoch: 7, Iter: 290, Loss: 7.83422\n",
            "[TRAIN] Epoch: 7, Iter: 300, Loss: 7.76867\n",
            "[TRAIN] Epoch: 7, Iter: 310, Loss: 7.83345\n",
            "[TRAIN] Epoch: 7, Iter: 320, Loss: 8.00469\n",
            "[TRAIN] Epoch: 7, Iter: 330, Loss: 7.82559\n",
            "[TRAIN] Epoch: 7, Iter: 340, Loss: 7.94697\n",
            "[TRAIN] Epoch: 7, Iter: 350, Loss: 7.92024\n",
            "[TRAIN] Epoch: 7, Iter: 360, Loss: 7.90420\n",
            "[TRAIN] Epoch: 7, Iter: 370, Loss: 7.97524\n",
            "[TRAIN] Epoch: 7, Iter: 380, Loss: 7.86483\n",
            "[TRAIN] Epoch: 7, Iter: 390, Loss: 7.94456\n",
            "[TRAIN] Epoch: 7, Iter: 400, Loss: 7.87442\n",
            "[TRAIN] Epoch: 7, Iter: 410, Loss: 7.88216\n",
            "[TRAIN] Epoch: 7, Iter: 420, Loss: 7.89055\n",
            "[TRAIN] Epoch: 7, Iter: 430, Loss: 8.02933\n",
            "[TRAIN] Epoch: 7, Iter: 440, Loss: 7.87511\n",
            "[TRAIN] Epoch: 7, Iter: 450, Loss: 7.91752\n",
            "[TRAIN] Epoch: 7, Iter: 460, Loss: 7.85270\n",
            "[TRAIN] Epoch: 7, Iter: 470, Loss: 7.89682\n",
            "[TRAIN] Epoch: 7, Iter: 480, Loss: 7.90392\n",
            "[TRAIN] Epoch: 7, Iter: 490, Loss: 7.73226\n",
            "[TRAIN] Epoch: 7, Iter: 500, Loss: 7.83751\n",
            "[TRAIN] Epoch: 7, Iter: 510, Loss: 7.89292\n",
            "[TRAIN] Epoch: 7, Iter: 520, Loss: 7.88928\n",
            "[TRAIN] Epoch: 7, Iter: 530, Loss: 7.98972\n",
            "[TRAIN] Epoch: 7, Iter: 540, Loss: 7.90985\n",
            "[TRAIN] Epoch: 7, Iter: 550, Loss: 7.81767\n",
            "[TRAIN] Epoch: 7, Iter: 560, Loss: 7.87991\n",
            "[TRAIN] Epoch: 7, Iter: 570, Loss: 7.96994\n",
            "== [TRAIN] Epoch: 7, Perplexity: 2691.155 ==>\n",
            "[VAL] Epoch: 7, Iter: 0, Loss: 7.89771\n",
            "[VAL] Epoch: 7, Iter: 10, Loss: 7.99575\n",
            "[VAL] Epoch: 7, Iter: 20, Loss: 7.96586\n",
            "[VAL] Epoch: 7, Iter: 30, Loss: 7.86584\n",
            "[VAL] Epoch: 7, Iter: 40, Loss: 7.76176\n",
            "[VAL] Epoch: 7, Iter: 50, Loss: 7.72493\n",
            "=== [VAL] Epoch: 7, Iter: 59, Perplexity: 2503.314 ===>\n",
            "====== Epoch 8 ======>\n",
            "[TRAIN] Epoch: 8, Iter: 0, Loss: 7.80810\n",
            "[TRAIN] Epoch: 8, Iter: 10, Loss: 7.86190\n",
            "[TRAIN] Epoch: 8, Iter: 20, Loss: 7.91059\n",
            "[TRAIN] Epoch: 8, Iter: 30, Loss: 7.79012\n",
            "[TRAIN] Epoch: 8, Iter: 40, Loss: 7.79827\n",
            "[TRAIN] Epoch: 8, Iter: 50, Loss: 7.82133\n",
            "[TRAIN] Epoch: 8, Iter: 60, Loss: 7.84981\n",
            "[TRAIN] Epoch: 8, Iter: 70, Loss: 7.86801\n",
            "[TRAIN] Epoch: 8, Iter: 80, Loss: 7.87864\n",
            "[TRAIN] Epoch: 8, Iter: 90, Loss: 7.81332\n",
            "[TRAIN] Epoch: 8, Iter: 100, Loss: 7.84109\n",
            "[TRAIN] Epoch: 8, Iter: 110, Loss: 7.89041\n",
            "[TRAIN] Epoch: 8, Iter: 120, Loss: 7.87284\n",
            "[TRAIN] Epoch: 8, Iter: 130, Loss: 7.82275\n",
            "[TRAIN] Epoch: 8, Iter: 140, Loss: 7.79317\n",
            "[TRAIN] Epoch: 8, Iter: 150, Loss: 7.85754\n",
            "[TRAIN] Epoch: 8, Iter: 160, Loss: 7.75473\n",
            "[TRAIN] Epoch: 8, Iter: 170, Loss: 7.85075\n",
            "[TRAIN] Epoch: 8, Iter: 180, Loss: 7.85104\n",
            "[TRAIN] Epoch: 8, Iter: 190, Loss: 7.69383\n",
            "[TRAIN] Epoch: 8, Iter: 200, Loss: 7.93415\n",
            "[TRAIN] Epoch: 8, Iter: 210, Loss: 7.78334\n",
            "[TRAIN] Epoch: 8, Iter: 220, Loss: 7.77402\n",
            "[TRAIN] Epoch: 8, Iter: 230, Loss: 7.83991\n",
            "[TRAIN] Epoch: 8, Iter: 240, Loss: 7.80114\n",
            "[TRAIN] Epoch: 8, Iter: 250, Loss: 7.90582\n",
            "[TRAIN] Epoch: 8, Iter: 260, Loss: 7.72842\n",
            "[TRAIN] Epoch: 8, Iter: 270, Loss: 7.76287\n",
            "[TRAIN] Epoch: 8, Iter: 280, Loss: 7.91808\n",
            "[TRAIN] Epoch: 8, Iter: 290, Loss: 7.88193\n",
            "[TRAIN] Epoch: 8, Iter: 300, Loss: 7.78833\n",
            "[TRAIN] Epoch: 8, Iter: 310, Loss: 7.83695\n",
            "[TRAIN] Epoch: 8, Iter: 320, Loss: 7.80070\n",
            "[TRAIN] Epoch: 8, Iter: 330, Loss: 7.79513\n",
            "[TRAIN] Epoch: 8, Iter: 340, Loss: 7.90855\n",
            "[TRAIN] Epoch: 8, Iter: 350, Loss: 7.84881\n",
            "[TRAIN] Epoch: 8, Iter: 360, Loss: 7.74255\n",
            "[TRAIN] Epoch: 8, Iter: 370, Loss: 7.78738\n",
            "[TRAIN] Epoch: 8, Iter: 380, Loss: 7.85692\n",
            "[TRAIN] Epoch: 8, Iter: 390, Loss: 7.69195\n",
            "[TRAIN] Epoch: 8, Iter: 400, Loss: 7.80248\n",
            "[TRAIN] Epoch: 8, Iter: 410, Loss: 7.85591\n",
            "[TRAIN] Epoch: 8, Iter: 420, Loss: 7.82077\n",
            "[TRAIN] Epoch: 8, Iter: 430, Loss: 7.83403\n",
            "[TRAIN] Epoch: 8, Iter: 440, Loss: 7.71872\n",
            "[TRAIN] Epoch: 8, Iter: 450, Loss: 7.88013\n",
            "[TRAIN] Epoch: 8, Iter: 460, Loss: 7.85306\n",
            "[TRAIN] Epoch: 8, Iter: 470, Loss: 7.75682\n",
            "[TRAIN] Epoch: 8, Iter: 480, Loss: 7.75663\n",
            "[TRAIN] Epoch: 8, Iter: 490, Loss: 7.77626\n",
            "[TRAIN] Epoch: 8, Iter: 500, Loss: 7.73513\n",
            "[TRAIN] Epoch: 8, Iter: 510, Loss: 7.78988\n",
            "[TRAIN] Epoch: 8, Iter: 520, Loss: 7.79999\n",
            "[TRAIN] Epoch: 8, Iter: 530, Loss: 7.75479\n",
            "[TRAIN] Epoch: 8, Iter: 540, Loss: 7.77601\n",
            "[TRAIN] Epoch: 8, Iter: 550, Loss: 7.82670\n",
            "[TRAIN] Epoch: 8, Iter: 560, Loss: 7.91339\n",
            "[TRAIN] Epoch: 8, Iter: 570, Loss: 7.78866\n",
            "== [TRAIN] Epoch: 8, Perplexity: 2485.494 ==>\n",
            "[VAL] Epoch: 8, Iter: 0, Loss: 7.82196\n",
            "[VAL] Epoch: 8, Iter: 10, Loss: 7.91373\n",
            "[VAL] Epoch: 8, Iter: 20, Loss: 7.89625\n",
            "[VAL] Epoch: 8, Iter: 30, Loss: 7.80722\n",
            "[VAL] Epoch: 8, Iter: 40, Loss: 7.68025\n",
            "[VAL] Epoch: 8, Iter: 50, Loss: 7.62895\n",
            "=== [VAL] Epoch: 8, Iter: 59, Perplexity: 2317.295 ===>\n",
            "====== Epoch 9 ======>\n",
            "[TRAIN] Epoch: 9, Iter: 0, Loss: 7.77715\n",
            "[TRAIN] Epoch: 9, Iter: 10, Loss: 7.84538\n",
            "[TRAIN] Epoch: 9, Iter: 20, Loss: 7.69648\n",
            "[TRAIN] Epoch: 9, Iter: 30, Loss: 7.72863\n",
            "[TRAIN] Epoch: 9, Iter: 40, Loss: 7.82313\n",
            "[TRAIN] Epoch: 9, Iter: 50, Loss: 7.89288\n",
            "[TRAIN] Epoch: 9, Iter: 60, Loss: 7.75157\n",
            "[TRAIN] Epoch: 9, Iter: 70, Loss: 7.72958\n",
            "[TRAIN] Epoch: 9, Iter: 80, Loss: 7.81324\n",
            "[TRAIN] Epoch: 9, Iter: 90, Loss: 7.84941\n",
            "[TRAIN] Epoch: 9, Iter: 100, Loss: 7.80503\n",
            "[TRAIN] Epoch: 9, Iter: 110, Loss: 7.66453\n",
            "[TRAIN] Epoch: 9, Iter: 120, Loss: 7.77847\n",
            "[TRAIN] Epoch: 9, Iter: 130, Loss: 7.79848\n",
            "[TRAIN] Epoch: 9, Iter: 140, Loss: 7.93277\n",
            "[TRAIN] Epoch: 9, Iter: 150, Loss: 7.70916\n",
            "[TRAIN] Epoch: 9, Iter: 160, Loss: 7.88762\n",
            "[TRAIN] Epoch: 9, Iter: 170, Loss: 7.85918\n",
            "[TRAIN] Epoch: 9, Iter: 180, Loss: 7.77302\n",
            "[TRAIN] Epoch: 9, Iter: 190, Loss: 7.73354\n",
            "[TRAIN] Epoch: 9, Iter: 200, Loss: 7.82127\n",
            "[TRAIN] Epoch: 9, Iter: 210, Loss: 7.79852\n",
            "[TRAIN] Epoch: 9, Iter: 220, Loss: 7.77674\n",
            "[TRAIN] Epoch: 9, Iter: 230, Loss: 7.78589\n",
            "[TRAIN] Epoch: 9, Iter: 240, Loss: 7.77827\n",
            "[TRAIN] Epoch: 9, Iter: 250, Loss: 7.75770\n",
            "[TRAIN] Epoch: 9, Iter: 260, Loss: 7.73202\n",
            "[TRAIN] Epoch: 9, Iter: 270, Loss: 7.87895\n",
            "[TRAIN] Epoch: 9, Iter: 280, Loss: 7.73458\n",
            "[TRAIN] Epoch: 9, Iter: 290, Loss: 7.73457\n",
            "[TRAIN] Epoch: 9, Iter: 300, Loss: 7.73553\n",
            "[TRAIN] Epoch: 9, Iter: 310, Loss: 7.70978\n",
            "[TRAIN] Epoch: 9, Iter: 320, Loss: 7.85104\n",
            "[TRAIN] Epoch: 9, Iter: 330, Loss: 7.81533\n",
            "[TRAIN] Epoch: 9, Iter: 340, Loss: 7.87993\n",
            "[TRAIN] Epoch: 9, Iter: 350, Loss: 7.79307\n",
            "[TRAIN] Epoch: 9, Iter: 360, Loss: 7.74958\n",
            "[TRAIN] Epoch: 9, Iter: 370, Loss: 7.71706\n",
            "[TRAIN] Epoch: 9, Iter: 380, Loss: 7.66329\n",
            "[TRAIN] Epoch: 9, Iter: 390, Loss: 7.67723\n",
            "[TRAIN] Epoch: 9, Iter: 400, Loss: 7.66142\n",
            "[TRAIN] Epoch: 9, Iter: 410, Loss: 7.78532\n",
            "[TRAIN] Epoch: 9, Iter: 420, Loss: 7.68686\n",
            "[TRAIN] Epoch: 9, Iter: 430, Loss: 7.74631\n",
            "[TRAIN] Epoch: 9, Iter: 440, Loss: 7.84057\n",
            "[TRAIN] Epoch: 9, Iter: 450, Loss: 7.78719\n",
            "[TRAIN] Epoch: 9, Iter: 460, Loss: 7.62214\n",
            "[TRAIN] Epoch: 9, Iter: 470, Loss: 7.71402\n",
            "[TRAIN] Epoch: 9, Iter: 480, Loss: 7.64363\n",
            "[TRAIN] Epoch: 9, Iter: 490, Loss: 7.72862\n",
            "[TRAIN] Epoch: 9, Iter: 500, Loss: 7.65848\n",
            "[TRAIN] Epoch: 9, Iter: 510, Loss: 7.76607\n",
            "[TRAIN] Epoch: 9, Iter: 520, Loss: 7.84798\n",
            "[TRAIN] Epoch: 9, Iter: 530, Loss: 7.74715\n",
            "[TRAIN] Epoch: 9, Iter: 540, Loss: 7.76928\n",
            "[TRAIN] Epoch: 9, Iter: 550, Loss: 7.67474\n",
            "[TRAIN] Epoch: 9, Iter: 560, Loss: 7.66501\n",
            "[TRAIN] Epoch: 9, Iter: 570, Loss: 7.60971\n",
            "== [TRAIN] Epoch: 9, Perplexity: 2322.209 ==>\n",
            "[VAL] Epoch: 9, Iter: 0, Loss: 7.75945\n",
            "[VAL] Epoch: 9, Iter: 10, Loss: 7.84476\n",
            "[VAL] Epoch: 9, Iter: 20, Loss: 7.83841\n",
            "[VAL] Epoch: 9, Iter: 30, Loss: 7.75902\n",
            "[VAL] Epoch: 9, Iter: 40, Loss: 7.61194\n",
            "[VAL] Epoch: 9, Iter: 50, Loss: 7.54737\n",
            "=== [VAL] Epoch: 9, Iter: 59, Perplexity: 2173.383 ===>\n",
            "[TEST] Epoch: 9, Iter: 0, Loss: 7.57184\n",
            "[TEST] Epoch: 9, Iter: 10, Loss: 7.70137\n",
            "[TEST] Epoch: 9, Iter: 20, Loss: 7.66593\n",
            "[TEST] Epoch: 9, Iter: 30, Loss: 7.69859\n",
            "[TEST] Epoch: 9, Iter: 40, Loss: 7.57941\n",
            "[TEST] Epoch: 9, Iter: 50, Loss: 7.79606\n",
            "[TEST] Epoch: 9, Iter: 60, Loss: 7.95436\n",
            "=== [TEST] Epoch: 9, Iter: 68, Perplexity: 2088.843 ===>\n",
            "===== Best validation perplexity: 2173.383 =====>\n"
          ]
        }
      ],
      "source": [
        "args = configs[3]  # Run the first configuration\n",
        "logs = main(args)\n",
        "if args.log:\n",
        "  save_logs(args, *logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwWZq6zk0YDF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}