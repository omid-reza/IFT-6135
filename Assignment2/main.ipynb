{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9VX-OHxC1FM"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qFHMMDtSwuW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136dd58b-a380-4164-8e43-016eed472f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Assignment2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradescope-utils\n",
        "!pip install tqdm\n",
        "!pip install GPUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvBMG-WamOAi",
        "outputId": "4cd10a3b-5fec-4032-f029-7a23c6a0d086"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradescope-utils\n",
            "  Downloading gradescope_utils-0.5.0-py2.py3-none-any.whl (7.1 kB)\n",
            "Installing collected packages: gradescope-utils\n",
            "Successfully installed gradescope-utils-0.5.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7395 sha256=77663d56988eebab1398a61317f6500f1a0afe7ae88b3f01d14690756266fbb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > ParamÃ¨tres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import urllib.request\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "from lstm_solution import LSTM\n",
        "from gpt1_solution import MiniGPT1\n",
        "from utils.wikitext2 import Wikitext2\n",
        "from utils.torch_utils import seed_experiment, to_device\n",
        "from utils.data_utils import save_logs\n",
        "from run_exp import train, evaluate\n",
        "import GPUtil\n",
        "\n",
        "EMBEDDINGS_URL = \"https://www.dropbox.com/s/g91502hubcmb4ob/embeddings.npz?dl=0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZr3Fh-qaGAZ"
      },
      "source": [
        "## Public tests\n",
        "Run the following cell in order to run the public tests to check to tensor shapes of the outputs of your functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GRwCZpSaaE9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1758475d-862c-4c32-dada-dd52f4e0771c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..........\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.540s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest discover -s /content/gdrive/MyDrive/Assignment2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PtvL_yKp3PW"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWiJme7XaLiR"
      },
      "source": [
        "Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularization, adding logs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YUrqebfCobD1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Arguments:\n",
        "  # Data\n",
        "  data_folder: str = '/content/gdrive/MyDrive/Assignment2/data'\n",
        "  batch_size: int = 16\n",
        "\n",
        "  # Model\n",
        "  model: str = 'lstm'  # [lstm, gpt1]\n",
        "  embeddings: str = '/content/gdrive/MyDrive/Assignment2/data/embeddings.npz'\n",
        "  layers: int = 1\n",
        "\n",
        "  # Optimization\n",
        "  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n",
        "  epochs: int = 10\n",
        "  lr: float = 1e-3\n",
        "  momentum: float = 0.9\n",
        "  weight_decay: float = 5e-4\n",
        "\n",
        "  # Experiment\n",
        "  exp_id: str = 'debug'\n",
        "  log: bool = True\n",
        "  log_dir: str = '/content/gdrive/MyDrive/Assignment2/logs'\n",
        "  seed: int = 42\n",
        "\n",
        "  # Miscellaneous\n",
        "  num_workers: int = 2\n",
        "  device: str = 'cuda'\n",
        "  progress_bar: bool = False\n",
        "  print_every: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ntfY6yyad_F"
      },
      "source": [
        "The 12 configurations you need to run in Problem 3. Be careful that there is no discrepency between the configurations defined in `run_exp.py` and the ones below. In case there is a difference, the version from `run_exp.py` should be considered the ones to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-q6AwUVDX78-"
      },
      "outputs": [],
      "source": [
        "# Note: if there is any discrepency with the configurations in run_exp.py, the\n",
        "# version from run_exp.py should be the ones to use in Problem 3.\n",
        "configs = {\n",
        "  1: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id=\"lstm_layer_1_btch_16_adam\"),\n",
        "  2: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_1_btch_16_adamw\"),\n",
        "  3: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id=\"lstm_layer_1_btch_16_sgd\"),\n",
        "  4: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id=\"lstm_layer_1_btch_16_momentum\"),\n",
        "\n",
        "  5: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id=\"gpt1_layer_1_btch_16_adam\"),\n",
        "  6: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_1_btch_16_adamw\"),\n",
        "  7: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id=\"gp1_layer_1_btch_16_sgd\"),\n",
        "  8: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id=\"gpt1_layer_1_btch_16_momentum\"),\n",
        "\n",
        "  9: Arguments(model='lstm', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_2_btch_16_adamw\"),\n",
        "  10: Arguments(model='lstm', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_4_btch_16_adamw\"),\n",
        "  11: Arguments(model='gpt1', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_2_btch_16_adamw\"),\n",
        "  12: Arguments(model='gpt1', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_4_btch_16_adamw\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_gpu_usage(file_path, exp_id, note=\"\"):\n",
        "    gpu_loads, gpu_free_memory, gpu_used_memory, gpu_total_memory = GPUtil.getGPUs()[0].load, GPUtil.getGPUs()[0].memoryFree, GPUtil.getGPUs()[0].memoryUsed, GPUtil.getGPUs()[0].memoryTotal\n",
        "    gpu_usage = f\"Load: {gpu_loads}, Free Memory: {gpu_free_memory}, Used Memory: {gpu_used_memory}, Total Memory: {gpu_total_memory}\"\n",
        "    with open(file_path, 'a+') as file:\n",
        "        file.write(f\"{exp_id}, {note}, {gpu_usage}\\n\")"
      ],
      "metadata": {
        "id": "thx4jllqnBp-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g2rjoY-5phTY"
      },
      "outputs": [],
      "source": [
        "def main(args, config_num_to_run):\n",
        "  # Seed the experiment, for repeatability\n",
        "  seed_experiment(args.seed)\n",
        "\n",
        "  # Dataloaders\n",
        "  train_dataset = Wikitext2(args.data_folder, split=\"train\")\n",
        "  train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  valid_dataset = Wikitext2(args.data_folder, split=\"validation\")\n",
        "  valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  test_dataset = Wikitext2(args.data_folder, split=\"test\")\n",
        "  test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  # Download the embeddings\n",
        "  if not os.path.isfile(args.embeddings):\n",
        "    print(\"Downloading embeddings...\")\n",
        "    urllib.request.urlretrieve(EMBEDDINGS_URL, args.embeddings)\n",
        "\n",
        "  # Model\n",
        "  if args.model == \"lstm\":\n",
        "    model = LSTM.load_embeddings_from(\n",
        "      args.embeddings, hidden_size=512, num_layers=args.layers\n",
        "    )\n",
        "  elif args.model == \"gpt1\":\n",
        "    model = MiniGPT1.load_embeddings_from(\n",
        "      args.embeddings, num_layers=args.layers\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(\"Unknown model {0}\".format(args.model))\n",
        "  model.to(args.device)\n",
        "\n",
        "  # Optimizer\n",
        "  if args.optimizer == \"adamw\":\n",
        "    optimizer = optim.AdamW(\n",
        "      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "  elif args.optimizer == \"adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "  elif args.optimizer == \"sgd\":\n",
        "    optimizer = optim.SGD(\n",
        "      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "  elif args.optimizer == \"momentum\":\n",
        "    optimizer = optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=args.lr,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.weight_decay,\n",
        "    )\n",
        "\n",
        "  print(\n",
        "    f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n",
        "    f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n",
        "  )\n",
        "  os.makedirs(\"/content/gdrive/MyDrive/Assignment2/log/\", exist_ok=True)\n",
        "  file_path = f\"/content/gdrive/MyDrive/Assignment2/log/gpu_config{config_num_to_run}.txt\"\n",
        "  log_gpu_usage(file_path, args.exp_id, \"Before Training\")\n",
        "  train_losses, valid_losses = [], []\n",
        "  train_ppls, valid_ppls = [], []\n",
        "  train_times, valid_times = [], []\n",
        "  for epoch in range(args.epochs):\n",
        "\n",
        "    tqdm.write(f\"====== Epoch {epoch} ======>\")\n",
        "\n",
        "    loss, ppl, wall_time = train(epoch, model, train_dataloader, optimizer, args)\n",
        "    train_losses.append(loss)\n",
        "    train_ppls.append(ppl)\n",
        "    train_times.append(wall_time)\n",
        "\n",
        "    loss, ppl, wall_time = evaluate(epoch, model, valid_dataloader, args)\n",
        "    valid_losses.append(loss)\n",
        "    valid_ppls.append(ppl)\n",
        "    valid_times.append(wall_time)\n",
        "    log_gpu_usage(file_path, args.exp_id, f\"After Epoch {epoch}\")\n",
        "  test_loss, test_ppl, test_time = evaluate(\n",
        "    epoch, model, test_dataloader, args, mode=\"test\"\n",
        "  )\n",
        "\n",
        "  print(f\"===== Best validation perplexity: {min(valid_ppls):.3f} =====>\")\n",
        "  log_gpu_usage(file_path, args.exp_id, \"After Training\")\n",
        "  return (\n",
        "    train_losses,\n",
        "    train_ppls,\n",
        "    train_times,\n",
        "    valid_losses,\n",
        "    valid_ppls,\n",
        "    valid_times,\n",
        "    test_loss,\n",
        "    test_ppl,\n",
        "    test_time,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZyJPWO1ppcTx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97c785cd-707c-45ff-d629-3b66e8796d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized LSTM model with 34107392 total parameters, of which 3019520 are learnable.\n",
            "====== Epoch 0 ======>\n",
            "[TRAIN] Epoch: 0, Iter: 0, Loss: 10.60759\n",
            "[TRAIN] Epoch: 0, Iter: 10, Loss: 8.51249\n",
            "[TRAIN] Epoch: 0, Iter: 20, Loss: 7.83956\n",
            "[TRAIN] Epoch: 0, Iter: 30, Loss: 7.61677\n",
            "[TRAIN] Epoch: 0, Iter: 40, Loss: 7.55536\n",
            "[TRAIN] Epoch: 0, Iter: 50, Loss: 7.57465\n",
            "[TRAIN] Epoch: 0, Iter: 60, Loss: 7.42348\n",
            "[TRAIN] Epoch: 0, Iter: 70, Loss: 7.38456\n",
            "[TRAIN] Epoch: 0, Iter: 80, Loss: 7.25691\n",
            "[TRAIN] Epoch: 0, Iter: 90, Loss: 7.32298\n",
            "[TRAIN] Epoch: 0, Iter: 100, Loss: 7.19855\n",
            "[TRAIN] Epoch: 0, Iter: 110, Loss: 6.99736\n",
            "[TRAIN] Epoch: 0, Iter: 120, Loss: 7.16838\n",
            "[TRAIN] Epoch: 0, Iter: 130, Loss: 6.89345\n",
            "[TRAIN] Epoch: 0, Iter: 140, Loss: 6.89140\n",
            "[TRAIN] Epoch: 0, Iter: 150, Loss: 6.70056\n",
            "[TRAIN] Epoch: 0, Iter: 160, Loss: 6.87493\n",
            "[TRAIN] Epoch: 0, Iter: 170, Loss: 6.82528\n",
            "[TRAIN] Epoch: 0, Iter: 180, Loss: 6.60782\n",
            "[TRAIN] Epoch: 0, Iter: 190, Loss: 6.67162\n",
            "[TRAIN] Epoch: 0, Iter: 200, Loss: 6.56262\n",
            "[TRAIN] Epoch: 0, Iter: 210, Loss: 6.45637\n",
            "[TRAIN] Epoch: 0, Iter: 220, Loss: 6.57290\n",
            "[TRAIN] Epoch: 0, Iter: 230, Loss: 6.42560\n",
            "[TRAIN] Epoch: 0, Iter: 240, Loss: 6.60569\n",
            "[TRAIN] Epoch: 0, Iter: 250, Loss: 6.50631\n",
            "[TRAIN] Epoch: 0, Iter: 260, Loss: 6.30148\n",
            "[TRAIN] Epoch: 0, Iter: 270, Loss: 6.41344\n",
            "[TRAIN] Epoch: 0, Iter: 280, Loss: 6.37355\n",
            "[TRAIN] Epoch: 0, Iter: 290, Loss: 6.36047\n",
            "[TRAIN] Epoch: 0, Iter: 300, Loss: 6.36298\n",
            "[TRAIN] Epoch: 0, Iter: 310, Loss: 6.32609\n",
            "[TRAIN] Epoch: 0, Iter: 320, Loss: 6.22541\n",
            "[TRAIN] Epoch: 0, Iter: 330, Loss: 6.23687\n",
            "[TRAIN] Epoch: 0, Iter: 340, Loss: 6.24699\n",
            "[TRAIN] Epoch: 0, Iter: 350, Loss: 6.24481\n",
            "[TRAIN] Epoch: 0, Iter: 360, Loss: 6.29572\n",
            "[TRAIN] Epoch: 0, Iter: 370, Loss: 6.13573\n",
            "[TRAIN] Epoch: 0, Iter: 380, Loss: 6.30085\n",
            "[TRAIN] Epoch: 0, Iter: 390, Loss: 6.25296\n",
            "[TRAIN] Epoch: 0, Iter: 400, Loss: 6.03558\n",
            "[TRAIN] Epoch: 0, Iter: 410, Loss: 6.19620\n",
            "[TRAIN] Epoch: 0, Iter: 420, Loss: 6.20040\n",
            "[TRAIN] Epoch: 0, Iter: 430, Loss: 6.14005\n",
            "[TRAIN] Epoch: 0, Iter: 440, Loss: 6.10617\n",
            "[TRAIN] Epoch: 0, Iter: 450, Loss: 6.21362\n",
            "[TRAIN] Epoch: 0, Iter: 460, Loss: 6.14963\n",
            "[TRAIN] Epoch: 0, Iter: 470, Loss: 6.03848\n",
            "[TRAIN] Epoch: 0, Iter: 480, Loss: 6.02247\n",
            "[TRAIN] Epoch: 0, Iter: 490, Loss: 6.05661\n",
            "[TRAIN] Epoch: 0, Iter: 500, Loss: 5.98033\n",
            "[TRAIN] Epoch: 0, Iter: 510, Loss: 5.96939\n",
            "[TRAIN] Epoch: 0, Iter: 520, Loss: 6.13907\n",
            "[TRAIN] Epoch: 0, Iter: 530, Loss: 5.82306\n",
            "[TRAIN] Epoch: 0, Iter: 540, Loss: 6.12205\n",
            "[TRAIN] Epoch: 0, Iter: 550, Loss: 5.91595\n",
            "[TRAIN] Epoch: 0, Iter: 560, Loss: 5.96440\n",
            "[TRAIN] Epoch: 0, Iter: 570, Loss: 5.95785\n",
            "== [TRAIN] Epoch: 0, Perplexity: 727.147 ==>\n",
            "[VAL] Epoch: 0, Iter: 0, Loss: 5.96259\n",
            "[VAL] Epoch: 0, Iter: 10, Loss: 6.02778\n",
            "[VAL] Epoch: 0, Iter: 20, Loss: 6.16410\n",
            "[VAL] Epoch: 0, Iter: 30, Loss: 6.04159\n",
            "[VAL] Epoch: 0, Iter: 40, Loss: 5.84070\n",
            "[VAL] Epoch: 0, Iter: 50, Loss: 5.66550\n",
            "=== [VAL] Epoch: 0, Iter: 59, Perplexity: 377.808 ===>\n",
            "[TEST] Epoch: 0, Iter: 0, Loss: 5.78834\n",
            "[TEST] Epoch: 0, Iter: 10, Loss: 6.01463\n",
            "[TEST] Epoch: 0, Iter: 20, Loss: 6.09053\n",
            "[TEST] Epoch: 0, Iter: 30, Loss: 6.13897\n",
            "[TEST] Epoch: 0, Iter: 40, Loss: 5.81433\n",
            "[TEST] Epoch: 0, Iter: 50, Loss: 6.01478\n",
            "[TEST] Epoch: 0, Iter: 60, Loss: 6.23172\n",
            "=== [TEST] Epoch: 0, Iter: 68, Perplexity: 370.118 ===>\n",
            "===== Best validation perplexity: 377.808 =====>\n",
            "Initialized LSTM model with 34107392 total parameters, of which 3019520 are learnable.\n",
            "====== Epoch 0 ======>\n",
            "[TRAIN] Epoch: 0, Iter: 0, Loss: 10.60759\n",
            "[TRAIN] Epoch: 0, Iter: 10, Loss: 8.51249\n",
            "[TRAIN] Epoch: 0, Iter: 20, Loss: 7.83956\n",
            "[TRAIN] Epoch: 0, Iter: 30, Loss: 7.61677\n",
            "[TRAIN] Epoch: 0, Iter: 40, Loss: 7.55534\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d3285d133462>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconfig_num_to_run\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_num_to_run\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Run the first configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_num_to_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-dc5dc8d71520>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, config_num_to_run)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"====== Epoch {epoch} ======>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwall_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtrain_ppls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/Assignment2/run_exp.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, dataloader, optimizer, args)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for config_num_to_run in range(1, 12+1):\n",
        "  print(f\"Cofig num that is gonna be executed: {config_num_to_run}\")\n",
        "  args = configs[config_num_to_run]  # Run the first configuration\n",
        "  logs = main(args, config_num_to_run)\n",
        "  if args.log:\n",
        "    save_logs(args, *logs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}