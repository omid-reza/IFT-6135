{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9VX-OHxC1FM"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "qFHMMDtSwuW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5dcedc-b7d4-42d8-c4fc-81d3273ee6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Assignment2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradescope-utils\n",
        "!pip install tqdm\n",
        "!pip install GPUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvBMG-WamOAi",
        "outputId": "1405235e-1220-4ecb-bda7-a3e1de7682e5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradescope-utils in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > Param√®tres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import urllib.request\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "from lstm_solution import LSTM\n",
        "from gpt1_solution import MiniGPT1\n",
        "from utils.wikitext2 import Wikitext2\n",
        "from utils.torch_utils import seed_experiment, to_device\n",
        "from utils.data_utils import save_logs\n",
        "from run_exp import train, evaluate\n",
        "import GPUtil\n",
        "\n",
        "EMBEDDINGS_URL = \"https://www.dropbox.com/s/g91502hubcmb4ob/embeddings.npz?dl=0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZr3Fh-qaGAZ"
      },
      "source": [
        "## Public tests\n",
        "Run the following cell in order to run the public tests to check to tensor shapes of the outputs of your functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "GRwCZpSaaE9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b06ed4a-e73d-438d-f038-3e35e9d5863e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..........\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.218s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest discover -s /content/gdrive/MyDrive/Assignment2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PtvL_yKp3PW"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWiJme7XaLiR"
      },
      "source": [
        "Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularization, adding logs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "YUrqebfCobD1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Arguments:\n",
        "  # Data\n",
        "  data_folder: str = '/content/gdrive/MyDrive/Assignment2/data'\n",
        "  batch_size: int = 16\n",
        "\n",
        "  # Model\n",
        "  model: str = 'lstm'  # [lstm, gpt1]\n",
        "  embeddings: str = '/content/gdrive/MyDrive/Assignment2/data/embeddings.npz'\n",
        "  layers: int = 1\n",
        "\n",
        "  # Optimization\n",
        "  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n",
        "  epochs: int = 10\n",
        "  lr: float = 1e-3\n",
        "  momentum: float = 0.9\n",
        "  weight_decay: float = 5e-4\n",
        "\n",
        "  # Experiment\n",
        "  exp_id: str = 'debug'\n",
        "  log: bool = True\n",
        "  log_dir: str = '/content/gdrive/MyDrive/Assignment2/logs'\n",
        "  seed: int = 42\n",
        "\n",
        "  # Miscellaneous\n",
        "  num_workers: int = 2\n",
        "  device: str = 'cuda'\n",
        "  progress_bar: bool = False\n",
        "  print_every: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ntfY6yyad_F"
      },
      "source": [
        "The 12 configurations you need to run in Problem 3. Be careful that there is no discrepency between the configurations defined in `run_exp.py` and the ones below. In case there is a difference, the version from `run_exp.py` should be considered the ones to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-q6AwUVDX78-"
      },
      "outputs": [],
      "source": [
        "# Note: if there is any discrepency with the configurations in run_exp.py, the\n",
        "# version from run_exp.py should be the ones to use in Problem 3.\n",
        "config_num_to_run = 1\n",
        "configs = {\n",
        "  1: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=1, optimizer='adam', exp_id=\"lstm_layer_1_btch_16_adam\"),\n",
        "  2: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_1_btch_16_adamw\"),\n",
        "  3: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id=\"lstm_layer_1_btch_16_sgd\"),\n",
        "  4: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id=\"lstm_layer_1_btch_16_momentum\"),\n",
        "\n",
        "  5: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id=\"gpt1_layer_1_btch_16_adam\"),\n",
        "  6: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_1_btch_16_adamw\"),\n",
        "  7: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id=\"gp1_layer_1_btch_16_sgd\"),\n",
        "  8: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id=\"gpt1_layer_1_btch_16_momentum\"),\n",
        "\n",
        "  9: Arguments(model='lstm', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_2_btch_16_adamw\"),\n",
        "  10: Arguments(model='lstm', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"lstm_layer_4_btch_16_adamw\"),\n",
        "  11: Arguments(model='gpt1', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_2_btch_16_adamw\"),\n",
        "  12: Arguments(model='gpt1', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id=\"gpt1_layer_4_btch_16_adamw\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_gpu_usage(file_path, exp_id, note=\"\"):\n",
        "    gpu_loads, gpu_free_memory, gpu_used_memory, gpu_total_memory = GPUtil.getGPUs()[0].load, GPUtil.getGPUs()[0].memoryFree, GPUtil.getGPUs()[0].memoryUsed, GPUtil.getGPUs()[0].memoryTotal\n",
        "    gpu_usage = f\"Load: {gpu_loads}, Free Memory: {gpu_free_memory}, Used Memory: {gpu_used_memory}, Total Memory: {gpu_total_memory}\"\n",
        "    with open(file_path, 'a+') as file:\n",
        "        file.write(f\"{exp_id}, {note}, {gpu_usage}\\n\")"
      ],
      "metadata": {
        "id": "thx4jllqnBp-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "g2rjoY-5phTY"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "  # Seed the experiment, for repeatability\n",
        "  seed_experiment(args.seed)\n",
        "\n",
        "  # Dataloaders\n",
        "  train_dataset = Wikitext2(args.data_folder, split=\"train\")\n",
        "  train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  valid_dataset = Wikitext2(args.data_folder, split=\"validation\")\n",
        "  valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  test_dataset = Wikitext2(args.data_folder, split=\"test\")\n",
        "  test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=args.num_workers,\n",
        "  )\n",
        "\n",
        "  # Download the embeddings\n",
        "  if not os.path.isfile(args.embeddings):\n",
        "    print(\"Downloading embeddings...\")\n",
        "    urllib.request.urlretrieve(EMBEDDINGS_URL, args.embeddings)\n",
        "\n",
        "  # Model\n",
        "  if args.model == \"lstm\":\n",
        "    model = LSTM.load_embeddings_from(\n",
        "      args.embeddings, hidden_size=512, num_layers=args.layers\n",
        "    )\n",
        "  elif args.model == \"gpt1\":\n",
        "    model = MiniGPT1.load_embeddings_from(\n",
        "      args.embeddings, num_layers=args.layers\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(\"Unknown model {0}\".format(args.model))\n",
        "  model.to(args.device)\n",
        "\n",
        "  # Optimizer\n",
        "  if args.optimizer == \"adamw\":\n",
        "    optimizer = optim.AdamW(\n",
        "      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "  elif args.optimizer == \"adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "  elif args.optimizer == \"sgd\":\n",
        "    optimizer = optim.SGD(\n",
        "      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
        "    )\n",
        "  elif args.optimizer == \"momentum\":\n",
        "    optimizer = optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=args.lr,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.weight_decay,\n",
        "    )\n",
        "\n",
        "  print(\n",
        "    f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n",
        "    f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n",
        "  )\n",
        "  os.makedirs(\"/content/gdrive/MyDrive/Assignment2/log/\", exist_ok=True)\n",
        "  file_path = f\"/content/gdrive/MyDrive/Assignment2/log/gpu_config{config_num_to_run}.txt\"\n",
        "  log_gpu_usage(file_path, args.exp_id, \"Before Training\")\n",
        "  train_losses, valid_losses = [], []\n",
        "  train_ppls, valid_ppls = [], []\n",
        "  train_times, valid_times = [], []\n",
        "  for epoch in range(args.epochs):\n",
        "\n",
        "    tqdm.write(f\"====== Epoch {epoch} ======>\")\n",
        "\n",
        "    loss, ppl, wall_time = train(epoch, model, train_dataloader, optimizer, args)\n",
        "    train_losses.append(loss)\n",
        "    train_ppls.append(ppl)\n",
        "    train_times.append(wall_time)\n",
        "\n",
        "    loss, ppl, wall_time = evaluate(epoch, model, valid_dataloader, args)\n",
        "    valid_losses.append(loss)\n",
        "    valid_ppls.append(ppl)\n",
        "    valid_times.append(wall_time)\n",
        "    log_gpu_usage(file_path, args.exp_id, f\"After Epoch {epoch}\")\n",
        "  test_loss, test_ppl, test_time = evaluate(\n",
        "    epoch, model, test_dataloader, args, mode=\"test\"\n",
        "  )\n",
        "\n",
        "  print(f\"===== Best validation perplexity: {min(valid_ppls):.3f} =====>\")\n",
        "  log_gpu_usage(file_path, args.exp_id, \"After Training\")\n",
        "  return (\n",
        "    train_losses,\n",
        "    train_ppls,\n",
        "    train_times,\n",
        "    valid_losses,\n",
        "    valid_ppls,\n",
        "    valid_times,\n",
        "    test_loss,\n",
        "    test_ppl,\n",
        "    test_time,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ZyJPWO1ppcTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a329f0-c797-4980-ee47-bea6e2f1a585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "for config_num_to_run in range(1, 12+1):\n",
        "  args = configs[config_num_to_run]  # Run the first configuration\n",
        "  logs = main(args)\n",
        "  if args.log:\n",
        "    save_logs(args, *logs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}